{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb73bd5",
   "metadata": {},
   "source": [
    "# Exploration et Modélisation pour le Score de Crédit Home Credit\n",
    "\n",
    "Ce notebook présente l'exploration des données et la modélisation pour le système de scoring de crédit de Home Credit.\n",
    "\n",
    "**Note**: Ce projet est basé sur le modèle de [Home Credit Default Risk par rakshithvasudev](https://github.com/rakshithvasudev/Home-Credit-Default-Risk) que nous allons adapter et faire évoluer pour notre cas d'utilisation.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "De nombreuses personnes ont des difficultés à obtenir des prêts en raison d'antécédents de crédit insuffisants ou inexistants. Malheureusement, cette population est souvent exploitée par des prêteurs peu scrupuleux.\n",
    "\n",
    "Home Credit s'efforce d'élargir l'inclusion financière pour la population non bancarisée en offrant une expérience d'emprunt positive et sûre. Pour s'assurer que cette population mal desservie vive une expérience de prêt positive, Home Credit utilise diverses données alternatives - y compris des informations sur les télécommunications et les transactions - pour prédire la capacité de remboursement de ses clients.\n",
    "\n",
    "L'objectif de ce projet est d'utiliser les données historiques des demandes de prêt pour prédire si un candidat sera en mesure de rembourser un prêt ou non."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5dd62",
   "metadata": {},
   "source": [
    "## Téléchargement des données depuis Kaggle\n",
    "\n",
    "Pour ce projet, nous utilisons le dataset de la compétition Kaggle 'Home Credit Default Risk'. Voici comment télécharger et préparer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faac77b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installation de l'API Kaggle si nécessaire\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7384b7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous devez configurer votre API Kaggle. Deux options:\n",
      "1. Téléchargez kaggle.json depuis votre compte Kaggle (My Account > API) et placez-le dans ~/.kaggle/\n",
      "2. Ou entrez vos identifiants ci-dessous (ils ne seront pas affichés):\n",
      "Identifiants Kaggle sauvegardés avec succès!\n",
      "Identifiants Kaggle sauvegardés avec succès!\n"
     ]
    }
   ],
   "source": [
    "# Configuration des identifiants Kaggle\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Créer le dossier Kaggle si nécessaire\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "\n",
    "# Vérifier si le fichier d'API Kaggle existe déjà\n",
    "kaggle_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
    "if not os.path.exists(kaggle_path):\n",
    "    print(\"Vous devez configurer votre API Kaggle. Deux options:\")\n",
    "    print(\"1. Téléchargez kaggle.json depuis votre compte Kaggle (My Account > API) et placez-le dans ~/.kaggle/\")\n",
    "    print(\"2. Ou entrez vos identifiants ci-dessous (ils ne seront pas affichés):\")\n",
    "    \n",
    "    import getpass\n",
    "    username = input(\"Nom d'utilisateur Kaggle: \")\n",
    "    key = getpass.getpass(\"Clé API Kaggle: \")\n",
    "    \n",
    "    # Sauvegarder les identifiants\n",
    "    with open(kaggle_path, 'w') as f:\n",
    "        json.dump({\"username\": username, \"key\": key}, f)\n",
    "    \n",
    "    # Protéger le fichier\n",
    "    os.chmod(kaggle_path, 0o600)\n",
    "    print(\"Identifiants Kaggle sauvegardés avec succès!\")\n",
    "else:\n",
    "    print(\"Fichier d'API Kaggle trouvé!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5105154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement du dataset Home Credit Default Risk...\n",
      "401 Client Error: Unauthorized for url: https://www.kaggle.com/api/v1/competitions/data/download-all/home-credit-default-risk\n",
      "401 Client Error: Unauthorized for url: https://www.kaggle.com/api/v1/competitions/data/download-all/home-credit-default-risk\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/home-credit-default-risk.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Extraire les fichiers zip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhome-credit-default-risk.zip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m     14\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(DATA_PATH)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDonnées téléchargées et extraites avec succès!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/zipfile/__init__.py:1331\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1331\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1333\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/home-credit-default-risk.zip'"
     ]
    }
   ],
   "source": [
    "# Définir le répertoire de données et télécharger le dataset si nécessaire\n",
    "DATA_PATH = \"../data/\"\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "# Vérifier si les données sont déjà téléchargées\n",
    "if not os.path.exists(os.path.join(DATA_PATH, \"application_train.csv\")):\n",
    "    print(\"Téléchargement du dataset Home Credit Default Risk...\")\n",
    "    # Télécharger les données depuis Kaggle\n",
    "    !kaggle competitions download -c home-credit-default-risk -p {DATA_PATH}\n",
    "    \n",
    "    # Extraire les fichiers zip\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(os.path.join(DATA_PATH, 'home-credit-default-risk.zip'), 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_PATH)\n",
    "        \n",
    "    print(\"Données téléchargées et extraites avec succès!\")\n",
    "else:\n",
    "    print(\"Les données sont déjà téléchargées et extraites.\")\n",
    "\n",
    "# Afficher les fichiers de données disponibles\n",
    "print(\"\\nFichiers de données disponibles:\")\n",
    "for file in sorted(os.listdir(DATA_PATH)):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(DATA_PATH, file)\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\" - {file} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperçu de la structure des données principales\n",
    "import pandas as pd\n",
    "\n",
    "# Charger les premières lignes des fichiers clés pour comprendre leur structure\n",
    "print(\"Structure du fichier application_train.csv:\")\n",
    "app_train = pd.read_csv(os.path.join(DATA_PATH, 'application_train.csv'), nrows=5)\n",
    "print(f\"Shape: {app_train.shape}\")\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b787552d",
   "metadata": {},
   "source": [
    "## Installation des dépendances spéciales pour macOS\n",
    "\n",
    "XGBoost nécessite la bibliothèque libomp sur macOS. Installons-la via Homebrew avant de continuer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd3bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Détection de macOS, installation de libomp pour XGBoost...\n",
      "/usr/local/bin/brew\n",
      "/usr/local/bin/brew\n",
      "Homebrew est installé, installation de libomp...\n",
      "Homebrew est installé, installation de libomp...\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updating Homebrew...\u001b[0m\n",
      "Adjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\n",
      "HOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updating Homebrew...\u001b[0m\n",
      "Adjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\n",
      "HOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/portable-ruby/portable-ruby/blobs/sha256:9fd394a40fb1467f89206a9c89c1274d9dc053af688176667a0cac0c3014113f\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/portable-ruby/portable-ruby/blobs/sha256:9fd394a40fb1467f89206a9c89c1274d9dc053af688176667a0cac0c3014113f\u001b[0m\n",
      "######################################################################### 100.0%###                                               38.6%###                                               38.6%############################        93.0%\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring portable-ruby-3.4.3.el_capitan.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring portable-ruby-3.4.3.el_capitan.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "Updated 4 taps (homebrew/services, mongodb/brew, symfony-cli/tap and homebrew/cask).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Casks\u001b[0m\n",
      "ace-studio                               font-source-han-code-jp\n",
      "advanced-renamer                         font-special-gothic\n",
      "agent-tars                               font-special-gothic-condensed-one\n",
      "antinote                                 font-special-gothic-expanded-one\n",
      "aqua-voice                               font-wdxl-lubrifont-jp-n\n",
      "atv-remote                               font-wdxl-lubrifont-sc\n",
      "automounterhelper                        font-wdxl-lubrifont-tc\n",
      "bambu-connect                            font-webdings\n",
      "batfi                                    font-winky-rough\n",
      "beutl                                    font-winky-sans\n",
      "block-goose                              fuse-t\n",
      "candy-crisis                             gologin\n",
      "captainplugins                           granola\n",
      "charles@4                                grayjay\n",
      "chime@alpha                              hamrs-pro\n",
      "chordpotion                              hedy\n",
      "cloudflare-warp@beta                     highlight\n",
      "cloudpouch                               hy-rpe2\n",
      "clover-chord-systems                     ijhttp\n",
      "comfyui                                  inmusic-software-center\n",
      "companion                                irpf2025\n",
      "companion-satellite                      istatistica-core\n",
      "companion@beta                           jumpcloud-password-manager\n",
      "consul                                   k6-studio\n",
      "coterm                                   kate\n",
      "dante-controller                         kilohearts-installer\n",
      "dante-via                                kunkun\n",
      "deepchat                                 liviable\n",
      "desktime                                 losslessswitcher\n",
      "earnapp                                  luanti\n",
      "elemental                                macskk\n",
      "elemental@6                              meru\n",
      "excire-foto                              mitti\n",
      "focu                                     moment\n",
      "foks                                     mouseless@preview\n",
      "font-adwaita                             ndi-tools\n",
      "font-adwaita-mono-nerd-font              nethlink\n",
      "font-ancizar-sans                        notion-mail\n",
      "font-ancizar-serif                       nvidia-nsight-compute\n",
      "font-aporetic                            obscura-vpn\n",
      "font-asta-sans                           opencloud\n",
      "font-atkynson-mono-nerd-font             opera-air\n",
      "font-big-shoulders                       outerbase-studio\n",
      "font-big-shoulders-inline                ovice\n",
      "font-big-shoulders-stencil               pairpods\n",
      "font-bizter                              pareto-security\n",
      "font-boldonse                            pastenow\n",
      "font-bytesized                           pdl\n",
      "font-comic-relief                        precize\n",
      "font-coral-pixels                        profit\n",
      "font-epunda-sans                         qobuz-downloader\n",
      "font-epunda-slab                         qt-design-studio\n",
      "font-exile                               rave\n",
      "font-fzhei-b01                           realvnc-connect\n",
      "font-fzxiheii-z08                        repo-prompt\n",
      "font-harmonyos-sans                      restapia\n",
      "font-harmonyos-sans-naskh-arabic         sc-menu\n",
      "font-harmonyos-sans-sc                   slidepad\n",
      "font-harmonyos-sans-tc                   sokim\n",
      "font-huninn                              soundanchor\n",
      "font-jetbrains-maple-mono                stability-matrix\n",
      "font-jetbrains-maple-mono-nf             swift-shift\n",
      "font-kumar-one-outline                   tal-drum\n",
      "font-libertinus-math                     thelowtechguys-cling\n",
      "font-libertinus-mono                     trae\n",
      "font-lxgw-wenkai-gb-lite                 trae-cn\n",
      "font-m-plus-rounded-1c                   triliumnext-notes\n",
      "font-maple-mono-normal                   ua-midi-control\n",
      "font-maple-mono-normal-cn                veracrypt-fuse-t\n",
      "font-maple-mono-normal-nf                vesktop\n",
      "font-maple-mono-normal-nf-cn             vezer\n",
      "font-noto-serif-dives-akuru              viables\n",
      "font-playpen-sans-arabic                 vimy\n",
      "font-playpen-sans-deva                   voiceink\n",
      "font-playpen-sans-hebrew                 warp@preview\n",
      "font-playpen-sans-thai                   windsurf@next\n",
      "font-pretendard-gov                      witsy\n",
      "font-sf-mono-nerd-font-ligaturized       yaak@beta\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "Updated 4 taps (homebrew/services, mongodb/brew, symfony-cli/tap and homebrew/cask).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Casks\u001b[0m\n",
      "ace-studio                               font-source-han-code-jp\n",
      "advanced-renamer                         font-special-gothic\n",
      "agent-tars                               font-special-gothic-condensed-one\n",
      "antinote                                 font-special-gothic-expanded-one\n",
      "aqua-voice                               font-wdxl-lubrifont-jp-n\n",
      "atv-remote                               font-wdxl-lubrifont-sc\n",
      "automounterhelper                        font-wdxl-lubrifont-tc\n",
      "bambu-connect                            font-webdings\n",
      "batfi                                    font-winky-rough\n",
      "beutl                                    font-winky-sans\n",
      "block-goose                              fuse-t\n",
      "candy-crisis                             gologin\n",
      "captainplugins                           granola\n",
      "charles@4                                grayjay\n",
      "chime@alpha                              hamrs-pro\n",
      "chordpotion                              hedy\n",
      "cloudflare-warp@beta                     highlight\n",
      "cloudpouch                               hy-rpe2\n",
      "clover-chord-systems                     ijhttp\n",
      "comfyui                                  inmusic-software-center\n",
      "companion                                irpf2025\n",
      "companion-satellite                      istatistica-core\n",
      "companion@beta                           jumpcloud-password-manager\n",
      "consul                                   k6-studio\n",
      "coterm                                   kate\n",
      "dante-controller                         kilohearts-installer\n",
      "dante-via                                kunkun\n",
      "deepchat                                 liviable\n",
      "desktime                                 losslessswitcher\n",
      "earnapp                                  luanti\n",
      "elemental                                macskk\n",
      "elemental@6                              meru\n",
      "excire-foto                              mitti\n",
      "focu                                     moment\n",
      "foks                                     mouseless@preview\n",
      "font-adwaita                             ndi-tools\n",
      "font-adwaita-mono-nerd-font              nethlink\n",
      "font-ancizar-sans                        notion-mail\n",
      "font-ancizar-serif                       nvidia-nsight-compute\n",
      "font-aporetic                            obscura-vpn\n",
      "font-asta-sans                           opencloud\n",
      "font-atkynson-mono-nerd-font             opera-air\n",
      "font-big-shoulders                       outerbase-studio\n",
      "font-big-shoulders-inline                ovice\n",
      "font-big-shoulders-stencil               pairpods\n",
      "font-bizter                              pareto-security\n",
      "font-boldonse                            pastenow\n",
      "font-bytesized                           pdl\n",
      "font-comic-relief                        precize\n",
      "font-coral-pixels                        profit\n",
      "font-epunda-sans                         qobuz-downloader\n",
      "font-epunda-slab                         qt-design-studio\n",
      "font-exile                               rave\n",
      "font-fzhei-b01                           realvnc-connect\n",
      "font-fzxiheii-z08                        repo-prompt\n",
      "font-harmonyos-sans                      restapia\n",
      "font-harmonyos-sans-naskh-arabic         sc-menu\n",
      "font-harmonyos-sans-sc                   slidepad\n",
      "font-harmonyos-sans-tc                   sokim\n",
      "font-huninn                              soundanchor\n",
      "font-jetbrains-maple-mono                stability-matrix\n",
      "font-jetbrains-maple-mono-nf             swift-shift\n",
      "font-kumar-one-outline                   tal-drum\n",
      "font-libertinus-math                     thelowtechguys-cling\n",
      "font-libertinus-mono                     trae\n",
      "font-lxgw-wenkai-gb-lite                 trae-cn\n",
      "font-m-plus-rounded-1c                   triliumnext-notes\n",
      "font-maple-mono-normal                   ua-midi-control\n",
      "font-maple-mono-normal-cn                veracrypt-fuse-t\n",
      "font-maple-mono-normal-nf                vesktop\n",
      "font-maple-mono-normal-nf-cn             vezer\n",
      "font-noto-serif-dives-akuru              viables\n",
      "font-playpen-sans-arabic                 vimy\n",
      "font-playpen-sans-deva                   voiceink\n",
      "font-playpen-sans-hebrew                 warp@preview\n",
      "font-playpen-sans-thai                   windsurf@next\n",
      "font-pretendard-gov                      witsy\n",
      "font-sf-mono-nerd-font-ligaturized       yaak@beta\n",
      "\n",
      "You have \u001b[1m85\u001b[0m outdated formulae installed.\n",
      "\n",
      "\n",
      "You have \u001b[1m85\u001b[0m outdated formulae installed.\n",
      "\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libomp/manifests/20.1.4\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libomp/manifests/20.1.4\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mlibomp\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libomp/blobs/sha256:9a78864f40e\u001b[0m\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mlibomp\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libomp/blobs/sha256:9a78864f40e\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libomp--20.1.4.sonoma.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libomp--20.1.4.sonoma.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mCaveats\u001b[0m\n",
      "libomp is keg-only, which means it was not symlinked into /usr/local,\n",
      "because it can override GCC headers and result in broken builds.\n",
      "\n",
      "For compilers to find libomp you may need to set:\n",
      "  export LDFLAGS=\"-L/usr/local/opt/libomp/lib\"\n",
      "  export CPPFLAGS=\"-I/usr/local/opt/libomp/include\"\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSummary\u001b[0m\n",
      "🍺  /usr/local/Cellar/libomp/20.1.4: 9 files, 1.7MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup libomp`...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mCaveats\u001b[0m\n",
      "libomp is keg-only, which means it was not symlinked into /usr/local,\n",
      "because it can override GCC headers and result in broken builds.\n",
      "\n",
      "For compilers to find libomp you may need to set:\n",
      "  export LDFLAGS=\"-L/usr/local/opt/libomp/lib\"\n",
      "  export CPPFLAGS=\"-I/usr/local/opt/libomp/include\"\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSummary\u001b[0m\n",
      "🍺  /usr/local/Cellar/libomp/20.1.4: 9 files, 1.7MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup libomp`...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n"
     ]
    }
   ],
   "source": [
    "# Vérifier si nous sommes sur macOS et installer libomp si nécessaire\n",
    "import platform\n",
    "import os\n",
    "\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    print(\"Détection de macOS, installation de libomp pour XGBoost...\")\n",
    "    # Vérifier si Homebrew est installé\n",
    "    try:\n",
    "        !which brew\n",
    "        print(\"Homebrew est installé, installation de libomp...\")\n",
    "        !brew install libomp\n",
    "    except:\n",
    "        print(\"Homebrew n'est pas installé. Veuillez installer Homebrew puis libomp:\")\n",
    "        print(\"1. /bin/bash -c \\\"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\\\"\")\n",
    "        print(\"2. brew install libomp\")\n",
    "else:\n",
    "    print(f\"Système d'exploitation détecté: {platform.system()}. Pas besoin d'installation spéciale pour XGBoost.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9a2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation de XGBoost avec les paramètres spécifiques pour macOS...\n",
      "Requirement already satisfied: xgboost in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from xgboost) (1.14.0)\n",
      "Requirement already satisfied: xgboost in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from xgboost) (1.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: lightgbm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.11.0)\n",
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: keras in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lightgbm) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.3.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.65.4)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: lightgbm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.11.0)\n",
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: keras in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lightgbm) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.3.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.65.4)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installation des bibliothèques nécessaires avec options spécifiques pour macOS\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    print(\"Installation de XGBoost avec les paramètres spécifiques pour macOS...\")\n",
    "    !pip install --no-binary xgboost xgboost\n",
    "    !pip install lightgbm h5py tensorflow keras\n",
    "else:\n",
    "    !pip install xgboost lightgbm h5py tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801062fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avertissement: imbalanced-learn n'est pas disponible. SMOTE sera désactivé.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 09:33:58.687074: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "import numpy as np  # pour les calculs mathématiques\n",
    "import pandas as pd  # pour la manipulation des données (csv)\n",
    "import matplotlib.pyplot as plt  # pour les graphiques\n",
    "import seaborn as sns  # pour plus d'options de graphiques (construit sur matplotlib)\n",
    "\n",
    "# Imports pour le machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer  # Remplace Imputer qui est obsolète\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Tenter d'importer XGBoost avec gestion d'erreurs\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"Avertissement: XGBoost n'a pas pu être importé: {e}\")\n",
    "    print(\"Les fonctionnalités utilisant XGBoost seront désactivées.\")\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "# Tenter d'importer LightGBM avec gestion d'erreurs\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGBM_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"Avertissement: LightGBM n'a pas pu être importé: {e}\")\n",
    "    print(\"Les fonctionnalités utilisant LightGBM seront désactivées.\")\n",
    "    LGBM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Avertissement: imbalanced-learn n'est pas disponible. SMOTE sera désactivé.\")\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Pour la sauvegarde de modèle au format h5\n",
    "import h5py\n",
    "import tempfile\n",
    "\n",
    "try:\n",
    "    from tensorflow import keras\n",
    "    import joblib\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Avertissement: TensorFlow/Keras n'est pas disponible. Le format h5 sera limité.\")\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "# Supprimer les avertissements inutiles pour que la présentation soit claire\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Afficher les graphiques dans le notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration pour afficher plus de colonnes\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53684a1",
   "metadata": {},
   "source": [
    "## Chargement des données\n",
    "\n",
    "Dans cette section, nous allons charger les données Home Credit. Dans un environnement de production, nous chargerions les fichiers de données réels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a8172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentative de chargement des données réelles de Home Credit...\n",
      "Erreur lors du chargement des données réelles: [Errno 2] No such file or directory: '../data/application_train.csv'\n",
      "Utilisation des données simulées pour le moment...\n"
     ]
    }
   ],
   "source": [
    "# Définition des chemins de fichiers\n",
    "DATA_PATH = \"../data/\"\n",
    "\n",
    "# Tenter de charger les données réelles depuis votre dossier local\n",
    "try:\n",
    "    print(\"Chargement des données locales depuis le dossier data...\")\n",
    "    train = pd.read_csv(os.path.join(DATA_PATH, 'application_train.csv'))\n",
    "    print(f\"Données d'entraînement chargées avec succès - {train.shape[0]} lignes et {train.shape[1]} colonnes\")\n",
    "    \n",
    "    if 'application_test.csv' in available_files:\n",
    "        test = pd.read_csv(os.path.join(DATA_PATH, 'application_test.csv'))\n",
    "        print(f\"Données de test chargées avec succès - {test.shape[0]} lignes et {test.shape[1]} colonnes\")\n",
    "    else:\n",
    "        # Créer un jeu de test à partir du jeu d'entraînement si le fichier test n'est pas disponible\n",
    "        print(\"Fichier application_test.csv non trouvé, création d'un échantillon de test à partir des données d'entraînement\")\n",
    "        # Séparer 20% des données d'entraînement pour le test\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train_data, test_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "        train = train_data.reset_index(drop=True)\n",
    "        test = test_data.drop('TARGET', axis=1).reset_index(drop=True)\n",
    "        print(f\"Jeu de test créé avec {test.shape[0]} lignes\")\n",
    "    \n",
    "    # Créer un petit échantillon du jeu de test pour les prédictions rapides\n",
    "    new_test = test.iloc[:100].copy()\n",
    "    print(f\"Échantillon de test pour nouvelles prédictions: {new_test.shape}\")\n",
    "    \n",
    "    print(\"\\nDonnées chargées avec succès!\")\n",
    "    DATA_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement des données: {e}\")\n",
    "    print(\"Utilisation des données simulées pour le moment...\")\n",
    "    DATA_AVAILABLE = False\n",
    "    \n",
    "    # Code pour générer des données simulées (inchangé)\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Identifiants clients\n",
    "    client_ids = np.arange(n_samples)\n",
    "    \n",
    "    # Caractéristiques des prêts\n",
    "    loan_amounts = np.random.normal(15000, 5000, n_samples)\n",
    "    loan_terms = np.random.choice([12, 24, 36, 48, 60], n_samples)\n",
    "    interest_rates = np.random.uniform(0.03, 0.15, n_samples)\n",
    "    \n",
    "    # Caractéristiques des clients\n",
    "    ages = np.random.normal(40, 10, n_samples).astype(int)\n",
    "    incomes = np.random.normal(50000, 15000, n_samples)\n",
    "    employment_years = np.random.normal(7, 4, n_samples)\n",
    "    debt_to_income = np.random.normal(0.3, 0.1, n_samples)\n",
    "    credit_scores = np.random.normal(650, 100, n_samples).astype(int)\n",
    "    \n",
    "    # Historique de prêts\n",
    "    num_previous_loans = np.random.poisson(2, n_samples)\n",
    "    num_delinquencies = np.random.poisson(0.5, n_samples)\n",
    "    \n",
    "    # Création de la variable cible (défaut de paiement)\n",
    "    probabilities = 1.0 / (1.0 + np.exp(-(0.0001 * loan_amounts - 0.05 * employment_years \n",
    "                                      + 0.1 * debt_to_income - 0.0001 * credit_scores \n",
    "                                      + 0.2 * num_delinquencies - 3)))\n",
    "    target = np.random.binomial(1, probabilities)\n",
    "    \n",
    "    # Création du DataFrame\n",
    "    train = pd.DataFrame({\n",
    "        'SK_ID_CURR': client_ids,\n",
    "        'TARGET': target,\n",
    "        'NAME_CONTRACT_TYPE': np.random.choice(['Cash loans', 'Revolving loans'], n_samples),\n",
    "        'CODE_GENDER': np.random.choice(['M', 'F'], n_samples),\n",
    "        'FLAG_OWN_CAR': np.random.choice(['Y', 'N'], n_samples),\n",
    "        'FLAG_OWN_REALTY': np.random.choice(['Y', 'N'], n_samples),\n",
    "        'CNT_CHILDREN': np.random.poisson(0.5, n_samples),\n",
    "        'AMT_INCOME_TOTAL': incomes,\n",
    "        'AMT_CREDIT': loan_amounts,\n",
    "        'AMT_ANNUITY': loan_amounts * (interest_rates / 12) * (1 + interest_rates / 12) ** loan_terms / ((1 + interest_rates / 12) ** loan_terms - 1),\n",
    "        'AMT_GOODS_PRICE': loan_amounts * 0.9,\n",
    "        'DAYS_BIRTH': -ages * 365,  # Convertir l'âge en jours négatifs\n",
    "        'DAYS_EMPLOYED': -employment_years * 365,  # Convertir les années en jours négatifs\n",
    "        'REGION_POPULATION_RELATIVE': np.random.uniform(0.001, 0.07, n_samples),\n",
    "        'EXT_SOURCE_1': np.random.uniform(0, 1, n_samples),\n",
    "        'EXT_SOURCE_2': np.random.uniform(0, 1, n_samples),\n",
    "        'EXT_SOURCE_3': np.random.uniform(0, 1, n_samples),\n",
    "    })\n",
    "    \n",
    "    # Création d'un jeu de test qui ressemble au jeu d'entraînement mais sans la colonne TARGET\n",
    "    test = train.copy().drop('TARGET', axis=1).iloc[:int(n_samples/5)]\n",
    "    \n",
    "    # Création d'un jeu de test 'new_test' pour les prédictions\n",
    "    new_test = train.copy().drop('TARGET', axis=1).iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour réduire l'utilisation de la mémoire\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" Réduit l'usage mémoire d'un DataFrame en convertissant les types de données \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Mémoire utilisée par le DataFrame: {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Mémoire utilisée après optimisation: {:.2f} MB'.format(end_mem))\n",
    "    print('Réduction de {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Optimiser l'utilisation de la mémoire si les données sont disponibles\n",
    "if DATA_AVAILABLE:\n",
    "    print(\"Optimisation de l'utilisation de la mémoire...\")\n",
    "    print(\"\\nOptimisation du jeu d'entraînement:\")\n",
    "    train = reduce_mem_usage(train)\n",
    "    print(\"\\nOptimisation du jeu de test:\")\n",
    "    test = reduce_mem_usage(test)\n",
    "    print(\"\\nOptimisation de l'échantillon de test:\")\n",
    "    new_test = reduce_mem_usage(new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6237369",
   "metadata": {},
   "source": [
    "## Exploration des données\n",
    "\n",
    "Examinons les données pour comprendre leur structure et leurs caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10a100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du jeu d'entraînement: (1000, 17)\n",
      "Dimensions du jeu de test: (200, 16)\n",
      "Dimensions du nouveau jeu de test: (100, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>40394.034814</td>\n",
       "      <td>17483.570765</td>\n",
       "      <td>395.709049</td>\n",
       "      <td>15735.213689</td>\n",
       "      <td>-17155</td>\n",
       "      <td>-2262.127190</td>\n",
       "      <td>0.059961</td>\n",
       "      <td>0.460813</td>\n",
       "      <td>0.783607</td>\n",
       "      <td>0.688727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>37740.568087</td>\n",
       "      <td>14308.678494</td>\n",
       "      <td>1246.627856</td>\n",
       "      <td>12877.810645</td>\n",
       "      <td>-14600</td>\n",
       "      <td>-3336.165980</td>\n",
       "      <td>0.039912</td>\n",
       "      <td>0.478720</td>\n",
       "      <td>0.197452</td>\n",
       "      <td>0.951414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>38521.664510</td>\n",
       "      <td>18238.442691</td>\n",
       "      <td>549.471041</td>\n",
       "      <td>16414.598421</td>\n",
       "      <td>-17155</td>\n",
       "      <td>-4071.847672</td>\n",
       "      <td>0.045788</td>\n",
       "      <td>0.177668</td>\n",
       "      <td>0.558449</td>\n",
       "      <td>0.562367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>50094.267873</td>\n",
       "      <td>22615.149282</td>\n",
       "      <td>429.273935</td>\n",
       "      <td>20353.634354</td>\n",
       "      <td>-11315</td>\n",
       "      <td>-3329.533758</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.876208</td>\n",
       "      <td>0.113097</td>\n",
       "      <td>0.960756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>13487.354212</td>\n",
       "      <td>13829.233126</td>\n",
       "      <td>409.084647</td>\n",
       "      <td>12446.309814</td>\n",
       "      <td>-19710</td>\n",
       "      <td>-3775.298760</td>\n",
       "      <td>0.026665</td>\n",
       "      <td>0.525821</td>\n",
       "      <td>0.361435</td>\n",
       "      <td>0.892455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0           0       1    Revolving loans           M            Y   \n",
       "1           1       0    Revolving loans           F            Y   \n",
       "2           2       0    Revolving loans           F            Y   \n",
       "3           3       0         Cash loans           F            Y   \n",
       "4           4       0    Revolving loans           F            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL    AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             1      40394.034814  17483.570765   395.709049   \n",
       "1               N             2      37740.568087  14308.678494  1246.627856   \n",
       "2               Y             0      38521.664510  18238.442691   549.471041   \n",
       "3               N             1      50094.267873  22615.149282   429.273935   \n",
       "4               Y             1      13487.354212  13829.233126   409.084647   \n",
       "\n",
       "   AMT_GOODS_PRICE  DAYS_BIRTH  DAYS_EMPLOYED  REGION_POPULATION_RELATIVE  \\\n",
       "0     15735.213689      -17155   -2262.127190                    0.059961   \n",
       "1     12877.810645      -14600   -3336.165980                    0.039912   \n",
       "2     16414.598421      -17155   -4071.847672                    0.045788   \n",
       "3     20353.634354      -11315   -3329.533758                    0.022200   \n",
       "4     12446.309814      -19710   -3775.298760                    0.026665   \n",
       "\n",
       "   EXT_SOURCE_1  EXT_SOURCE_2  EXT_SOURCE_3  \n",
       "0      0.460813      0.783607      0.688727  \n",
       "1      0.478720      0.197452      0.951414  \n",
       "2      0.177668      0.558449      0.562367  \n",
       "3      0.876208      0.113097      0.960756  \n",
       "4      0.525821      0.361435      0.892455  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examiner les dimensions des jeux de données\n",
    "print(\"Dimensions du jeu d'entraînement:\", train.shape)\n",
    "print(\"Dimensions du jeu de test:\", test.shape)\n",
    "print(\"Dimensions du nouveau jeu de test:\", new_test.shape)\n",
    "\n",
    "# Afficher les premières lignes du jeu d'entraînement\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e721ac02",
   "metadata": {},
   "source": [
    "## 1. Chargement des données\n",
    "\n",
    "Dans cette section, nous allons charger les données de Home Credit. Les données sont dispersées dans plusieurs tables qu'il faudra joindre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d21aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes avec valeurs manquantes dans le jeu d'entraînement:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Count</th>\n",
       "      <th>Missing Count Ratio</th>\n",
       "      <th>Missing Count %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Count, Missing Count Ratio, Missing Count %]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction pour trouver les colonnes avec des valeurs manquantes\n",
    "def missing_columns(dataframe):\n",
    "    \"\"\"\n",
    "    Renvoie un dataframe contenant les noms des colonnes manquantes et \n",
    "    le pourcentage de valeurs manquantes par rapport à l'ensemble du dataframe.\n",
    "    \n",
    "    dataframe: dataframe qui donne les noms des colonnes et leur % de valeurs manquantes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Trouver les valeurs manquantes\n",
    "    missing_values = dataframe.isnull().sum().sort_values(ascending=False)\n",
    "    \n",
    "    # Pourcentage de valeurs manquantes par rapport à la taille totale\n",
    "    missing_values_pct = 100 * missing_values/len(dataframe)\n",
    "    \n",
    "    # Créer un nouveau dataframe qui est une version concaténée\n",
    "    concat_values = pd.concat([missing_values, missing_values/len(dataframe), missing_values_pct.round(1)], axis=1)\n",
    "\n",
    "    # Donner de nouveaux noms de colonne\n",
    "    concat_values.columns = ['Missing Count', 'Missing Count Ratio', 'Missing Count %']\n",
    "    \n",
    "    # Retourner les valeurs requises\n",
    "    return concat_values[concat_values.iloc[:,1]!=0]\n",
    "    \n",
    "# Afficher les colonnes avec des valeurs manquantes\n",
    "print(\"Colonnes avec valeurs manquantes dans le jeu d'entraînement:\")\n",
    "missing_columns(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98b761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes avec valeurs manquantes dans le jeu d'entraînement:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Count</th>\n",
       "      <th>Missing Count Ratio</th>\n",
       "      <th>Missing Count %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Count, Missing Count Ratio, Missing Count %]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction pour trouver les colonnes avec des valeurs manquantes\n",
    "def missing_columns(dataframe):\n",
    "    \"\"\"\n",
    "    Renvoie un dataframe contenant les noms des colonnes manquantes et \n",
    "    le pourcentage de valeurs manquantes par rapport à l'ensemble du dataframe.\n",
    "    \n",
    "    dataframe: dataframe qui donne les noms des colonnes et leur % de valeurs manquantes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Trouver les valeurs manquantes\n",
    "    missing_values = dataframe.isnull().sum().sort_values(ascending=False)\n",
    "    \n",
    "    # Pourcentage de valeurs manquantes par rapport à la taille totale\n",
    "    missing_values_pct = 100 * missing_values/len(dataframe)\n",
    "    \n",
    "    # Créer un nouveau dataframe qui est une version concaténée\n",
    "    concat_values = pd.concat([missing_values, missing_values/len(dataframe), missing_values_pct.round(1)], axis=1)\n",
    "\n",
    "    # Donner de nouveaux noms de colonne\n",
    "    concat_values.columns = ['Missing Count', 'Missing Count Ratio', 'Missing Count %']\n",
    "    \n",
    "    # Retourner les valeurs requises\n",
    "    return concat_values[concat_values.iloc[:,1]!=0]\n",
    "    \n",
    "# Afficher les colonnes avec des valeurs manquantes\n",
    "print(\"Colonnes avec valeurs manquantes dans le jeu d'entraînement:\")\n",
    "missing_columns(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e98d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean       39.553000\n",
       "std        10.095995\n",
       "min         7.000000\n",
       "25%        33.000000\n",
       "50%        40.000000\n",
       "75%        46.000000\n",
       "max        72.000000\n",
       "Name: DAYS_BIRTH, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explorer l'âge des clients\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.distplot(train['DAYS_BIRTH'] / -365, bins=25, kde=False)\n",
    "plt.xlabel(\"Âge du client (années)\")\n",
    "plt.title('Distribution des âges')\n",
    "plt.show()\n",
    "\n",
    "# Statistiques de l'âge\n",
    "(train['DAYS_BIRTH'] / -365).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0876f50",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Créons de nouvelles caractéristiques qui pourraient être utiles pour la prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab703be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'anomalies avec 1000 ans d'emploi: 0\n",
      "Pas assez d'anomalies pour calculer le taux de défaut\n",
      "Non-anomalies ont un taux de défaut de 15.90%\n"
     ]
    }
   ],
   "source": [
    "# Vérifier les anomalies dans DAYS_EMPLOYED\n",
    "thousand_anomalies = train[(train['DAYS_EMPLOYED']/365 >= 900) & (train['DAYS_EMPLOYED']/365 <= 1100)]\n",
    "print(f\"Nombre d'anomalies avec 1000 ans d'emploi: {len(thousand_anomalies)}\")\n",
    "\n",
    "# Comparer le taux de défaut entre les anomalies et les non-anomalies\n",
    "anomalies_index = pd.Index(thousand_anomalies.index)\n",
    "non_anomalies_index = train.index.difference(anomalies_index)\n",
    "non_anomalies = train.iloc[non_anomalies_index]\n",
    "\n",
    "anomalies_target = thousand_anomalies['TARGET'].value_counts()\n",
    "non_anomalies_target = non_anomalies['TARGET'].value_counts()\n",
    "\n",
    "if len(anomalies_target) > 1 and 1 in anomalies_target and 0 in anomalies_target:\n",
    "    print(f\"Anomalies ont un taux de défaut de {100*anomalies_target[1]/(anomalies_target[1]+anomalies_target[0]):.2f}%\")\n",
    "else:\n",
    "    print(\"Pas assez d'anomalies pour calculer le taux de défaut\")\n",
    "    \n",
    "if len(non_anomalies_target) > 1 and 1 in non_anomalies_target and 0 in non_anomalies_target:\n",
    "    print(f\"Non-anomalies ont un taux de défaut de {100*non_anomalies_target[1]/(non_anomalies_target[1]+non_anomalies_target[0]):.2f}%\")\n",
    "else:\n",
    "    print(\"Pas assez de non-anomalies pour calculer le taux de défaut\")\n",
    "\n",
    "# Créer une colonne indicatrice d'anomalie et remplacer les valeurs anomaliques par NaN\n",
    "train['DAYS_EMPLOYED_ANOM'] = train[\"DAYS_EMPLOYED\"] == 365243\n",
    "train['DAYS_EMPLOYED'] = train['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "\n",
    "# Faire de même pour les jeux de test\n",
    "test['DAYS_EMPLOYED_ANOM'] = test[\"DAYS_EMPLOYED\"] == 365243\n",
    "test['DAYS_EMPLOYED'] = test['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "\n",
    "new_test['DAYS_EMPLOYED_ANOM'] = new_test[\"DAYS_EMPLOYED\"] == 365243\n",
    "new_test['DAYS_EMPLOYED'] = new_test['DAYS_EMPLOYED'].replace({365243: np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965fb2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Revolving loans'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Explorer les corrélations avec la variable cible\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m corr_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTARGET\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msort_values()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Afficher les 10 variables les plus négativement corrélées\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariables les plus négativement corrélées à TARGET:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:11049\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[0;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[1;32m  11047\u001b[0m cols \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m  11048\u001b[0m idx \u001b[38;5;241m=\u001b[39m cols\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m> 11049\u001b[0m mat \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m  11051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m  11052\u001b[0m     correl \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mnancorr(mat, minp\u001b[38;5;241m=\u001b[39mmin_periods)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:1993\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1992\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[0;32m-> 1993\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[1;32m   1995\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(result, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/managers.py:1694\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1692\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1694\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/managers.py:1753\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[0;34m(self, dtype, na_value)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m         arr \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mget_values(dtype)\n\u001b[0;32m-> 1753\u001b[0m     \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m arr\n\u001b[1;32m   1754\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Revolving loans'"
     ]
    }
   ],
   "source": [
    "# Explorer les corrélations avec la variable cible\n",
    "# Utiliser numeric_only=True pour éviter l'erreur avec les colonnes non numériques\n",
    "corr_train = train.corr(numeric_only=True)['TARGET'].sort_values()\n",
    "\n",
    "# Afficher les 10 variables les plus négativement corrélées\n",
    "print(\"Variables les plus négativement corrélées à TARGET:\")\n",
    "print(corr_train.head(10))\n",
    "\n",
    "# Afficher les 10 variables les plus positivement corrélées\n",
    "print(\"\\nVariables les plus positivement corrélées à TARGET:\")\n",
    "print(corr_train.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corrélation pour les variables numériques\n",
    "plt.figure(figsize=(16, 14))\n",
    "# Sélectionner uniquement les colonnes numériques\n",
    "numeric_vars = train.select_dtypes(include=[np.number]).columns\n",
    "# Limiter à 20 variables pour une meilleure lisibilité\n",
    "selected_vars = list(numeric_vars[:20])\n",
    "if 'TARGET' in train.columns and 'TARGET' not in selected_vars:\n",
    "    selected_vars.append('TARGET')\n",
    "sns.heatmap(train[selected_vars].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Matrice de corrélation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorer les corrélations avec la variable cible\n",
    "# Utiliser numeric_only=True pour éviter l'erreur avec les colonnes non numériques\n",
    "corr_train = train.corr(numeric_only=True)['TARGET'].sort_values()\n",
    "\n",
    "# Afficher les 10 variables les plus négativement corrélées\n",
    "print(\"Variables les plus négativement corrélées à TARGET:\")\n",
    "print(corr_train.head(10))\n",
    "\n",
    "# Afficher les 10 variables les plus positivement corrélées\n",
    "print(\"\\nVariables les plus positivement corrélées à TARGET:\")\n",
    "print(corr_train.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962711e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation des valeurs manquantes et mise à l'échelle des données\n",
    "\n",
    "# Séparer les variables et la cible\n",
    "features = [col for col in train.columns if col != 'TARGET']\n",
    "target = train['TARGET'].values\n",
    "\n",
    "# Remplacer les valeurs infinies par 0\n",
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "test = test.replace([np.inf, -np.inf], np.nan)\n",
    "new_test = new_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Créer un SimpleImputer pour remplacer les valeurs manquantes par la médiane\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(train[features])\n",
    "\n",
    "# Transformer les jeux de données\n",
    "train_transformed = imputer.transform(train[features])\n",
    "test_transformed = imputer.transform(test[features])\n",
    "new_test_transformed = imputer.transform(new_test)\n",
    "\n",
    "# Mise à l'échelle des données\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_transformed = scaler.fit_transform(train_transformed)\n",
    "test_transformed = scaler.transform(test_transformed)\n",
    "new_test_transformed = scaler.transform(new_test_transformed)\n",
    "\n",
    "print(f\"Dimensions après transformation:\")\n",
    "print(f\"Train transformé: {train_transformed.shape}\")\n",
    "print(f\"Test transformé: {test_transformed.shape}\")\n",
    "print(f\"New test transformé: {new_test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de variables basées sur l'expertise du domaine\n",
    "# Ces variables sont mentionnées dans l'article de Wells Fargo sur les facteurs de crédit\n",
    "\n",
    "# debt-to-income ratio (DIR) = Montant du crédit / Revenu total\n",
    "train['DIR'] = train['AMT_CREDIT'] / train['AMT_INCOME_TOTAL']\n",
    "test['DIR'] = test['AMT_CREDIT'] / test['AMT_INCOME_TOTAL']\n",
    "new_test['DIR'] = new_test['AMT_CREDIT'] / new_test['AMT_INCOME_TOTAL']\n",
    "\n",
    "# annuity-to-income ratio (AIR) = Annuité du prêt / Revenu total\n",
    "train['AIR'] = train['AMT_ANNUITY'] / train['AMT_INCOME_TOTAL']\n",
    "test['AIR'] = test['AMT_ANNUITY'] / test['AMT_INCOME_TOTAL']\n",
    "new_test['AIR'] = new_test['AMT_ANNUITY'] / new_test['AMT_INCOME_TOTAL']\n",
    "\n",
    "# annuity-to-credit ratio (ACR) = Annuité du prêt / Montant du crédit\n",
    "train['ACR'] = train['AMT_ANNUITY'] / train['AMT_CREDIT']\n",
    "test['ACR'] = test['AMT_ANNUITY'] / test['AMT_CREDIT']\n",
    "new_test['ACR'] = new_test['AMT_ANNUITY'] / new_test['AMT_CREDIT']\n",
    "\n",
    "# days-employed-to-age ratio (DAR) = Jours employés / Âge du demandeur\n",
    "train['DAR'] = train['DAYS_EMPLOYED'] / train['DAYS_BIRTH']\n",
    "test['DAR'] = test['DAYS_EMPLOYED'] / test['DAYS_BIRTH']\n",
    "new_test['DAR'] = new_test['DAYS_EMPLOYED'] / new_test['DAYS_BIRTH']\n",
    "\n",
    "# Vérifier les corrélations des nouvelles variables avec TARGET\n",
    "domain_features = ['DIR', 'AIR', 'ACR', 'DAR']\n",
    "domain_corrs = train[domain_features + ['TARGET']].corr(numeric_only=True)['TARGET']\n",
    "print(\"Corrélations des variables d'expertise de domaine avec TARGET:\")\n",
    "print(domain_corrs[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ee09c",
   "metadata": {},
   "source": [
    "## Préparation des données pour la modélisation\n",
    "\n",
    "Avant de construire des modèles, nous devons préparer les données en encodant les variables catégorielles et en traitant les valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder les variables catégorielles avec 2 catégories ou moins\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "transform_counter = 0\n",
    "\n",
    "# Vérifier si le jeu de données contient des variables de type 'object'\n",
    "if 'object' in train.dtypes.values:\n",
    "    # Itérer à travers toutes les colonnes catégorielles\n",
    "    for col in train.select_dtypes('object').columns:\n",
    "        # Sélectionner uniquement les colonnes où le nombre de valeurs uniques est inférieur ou égal à 2\n",
    "        if train[col].nunique() <= 2:\n",
    "            train[col] = le.fit_transform(train[col].astype(str))\n",
    "            # Vérifier si la colonne existe dans les jeux de test\n",
    "            if col in test.columns:\n",
    "                test[col] = le.transform(test[col].astype(str))\n",
    "            if col in new_test.columns:\n",
    "                new_test[col] = le.transform(new_test[col].astype(str))\n",
    "            transform_counter += 1\n",
    "    \n",
    "    print(f\"Encodage par étiquettes appliqué à {transform_counter} colonnes.\")\n",
    "    \n",
    "    # One-hot encoding pour les variables catégorielles restantes\n",
    "    train = pd.get_dummies(train, drop_first=True)\n",
    "    test = pd.get_dummies(test, drop_first=True)\n",
    "    new_test = pd.get_dummies(new_test, drop_first=True)\n",
    "    \n",
    "    # Aligner les colonnes entre train et test\n",
    "    target = train['TARGET']\n",
    "    train, test = train.align(test, join='inner', axis=1)\n",
    "    train['TARGET'] = target\n",
    "    \n",
    "    # S'assurer que new_test a les mêmes colonnes que train\n",
    "    for col in train.columns:\n",
    "        if col != 'TARGET' and col not in new_test.columns:\n",
    "            new_test[col] = 0\n",
    "    new_test = new_test[train.columns[train.columns != 'TARGET']]\n",
    "    \n",
    "    print(f\"Dimensions après encodage one-hot:\")\n",
    "    print(f\"Train: {train.shape}\")\n",
    "    print(f\"Test: {test.shape}\")\n",
    "    print(f\"New test: {new_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corrélation pour les variables numériques\n",
    "plt.figure(figsize=(16, 14))\n",
    "numeric_vars = train.select_dtypes(include=[np.number]).columns\n",
    "# Limiter à 20 variables pour une meilleure lisibilité\n",
    "selected_vars = list(numeric_vars[:20])\n",
    "if 'TARGET' in train.columns and 'TARGET' not in selected_vars:\n",
    "    selected_vars.append('TARGET')\n",
    "sns.heatmap(train[selected_vars].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Matrice de corrélation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc465ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les prédictions de probabilité pour la soumission\n",
    "log_regression_pred_test = logistic_regressor.predict_proba(test_transformed)[:, 1]\n",
    "\n",
    "# Créer un DataFrame pour la soumission\n",
    "submission_log_regression = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': log_regression_pred_test})\n",
    "submission_log_regression.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c833d3",
   "metadata": {},
   "source": [
    "## Modélisation\n",
    "\n",
    "Nous allons entraîner plusieurs modèles de classification et évaluer leurs performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle XGBoost (conditionnel)\n",
    "if 'XGB_AVAILABLE' in globals() and XGB_AVAILABLE:\n",
    "    xgb_classifier = XGBClassifier(n_estimators=250, max_depth=5)\n",
    "    xgb_classifier.fit(X_training_set, y_training_set)\n",
    "    \n",
    "    # Prédictions sur l'ensemble de validation\n",
    "    xgb_pred = xgb_classifier.predict(X_validation_set)\n",
    "    xgb_pred_proba = xgb_classifier.predict_proba(X_validation_set)[:, 1]\n",
    "    \n",
    "    # Évaluation du modèle\n",
    "    print(\"XGBoost - Rapport de classification:\")\n",
    "    print(classification_report(y_validation_set, xgb_pred))\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_validation_set, xgb_pred_proba):.4f}\")\n",
    "    \n",
    "    # Prédictions sur le nouveau jeu de test\n",
    "    xgb_new = xgb_classifier.predict(new_test_transformed)\n",
    "    print(\"\\nDistribution des prédictions sur le nouveau jeu de test:\")\n",
    "    print(pd.Series(xgb_new).value_counts())\n",
    "else:\n",
    "    print(\"XGBoost n'est pas disponible. Passage à l'étape suivante.\")\n",
    "    # Définir des valeurs vides pour ne pas casser le code plus tard\n",
    "    xgb_classifier = None\n",
    "    xgb_pred = np.zeros(len(y_validation_set))\n",
    "    xgb_pred_proba = np.zeros(len(y_validation_set))\n",
    "    xgb_new = np.zeros(len(new_test_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'importance des caractéristiques du modèle XGBoost\n",
    "xgb_importance = plot_importance(xgb_classifier, features, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0563ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les prédictions de probabilité pour la soumission\n",
    "log_regression_pred_test = logistic_regressor.predict_proba(test_transformed)[:, 1]\n",
    "\n",
    "# Créer un DataFrame pour la soumission\n",
    "submission_log_regression = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': log_regression_pred_test})\n",
    "submission_log_regression.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49a22b",
   "metadata": {},
   "source": [
    "### 2. Random Forest (Forêt aléatoire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e56692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n",
    "random_forest.fit(X_training_set, y_training_set)\n",
    "\n",
    "# Prédictions sur l'ensemble de validation\n",
    "random_forest_pred = random_forest.predict(X_validation_set)\n",
    "random_forest_pred_proba = random_forest.predict_proba(X_validation_set)[:, 1]\n",
    "\n",
    "# Évaluation du modèle\n",
    "print(\"Random Forest - Rapport de classification:\")\n",
    "print(classification_report(y_validation_set, random_forest_pred))\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_validation_set, random_forest_pred_proba):.4f}\")\n",
    "\n",
    "# Prédictions sur le nouveau jeu de test\n",
    "random_forest_new = random_forest.predict(new_test_transformed)\n",
    "print(\"\\nDistribution des prédictions sur le nouveau jeu de test:\")\n",
    "print(pd.Series(random_forest_new).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbd59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser l'importance des caractéristiques\n",
    "def plot_importance(model, features, top_n=20):\n",
    "    \"\"\"Affiche un graphique des caractéristiques les plus importantes.\"\"\"\n",
    "    # Créer un DataFrame pour l'importance des caractéristiques\n",
    "    feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': model.feature_importances_})\n",
    "    feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Créer le graphique\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(top_n))\n",
    "    plt.title(f\"Les {top_n} caractéristiques les plus importantes\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importance_df\n",
    "\n",
    "# Visualiser l'importance des caractéristiques du modèle Random Forest\n",
    "rf_importance = plot_importance(random_forest, features, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f45538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier l'importance des caractéristiques créées par expertise de domaine\n",
    "domain_features = ['DIR', 'AIR', 'ACR', 'DAR']\n",
    "domain_indices = [i for i, feature in enumerate(features) if feature in domain_features]\n",
    "\n",
    "if domain_indices:\n",
    "    domain_importance = pd.DataFrame({\n",
    "        'Feature': [features[i] for i in domain_indices],\n",
    "        'Importance': [random_forest.feature_importances_[i] for i in domain_indices]\n",
    "    })\n",
    "    domain_importance = domain_importance.sort_values('Importance', ascending=False)\n",
    "    print(\"Importance des caractéristiques créées par expertise de domaine:\")\n",
    "    print(domain_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baffdd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les prédictions pour la soumission\n",
    "random_forest_pred_test = random_forest.predict_proba(test_transformed)[:, 1]\n",
    "submission_rf = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': random_forest_pred_test})\n",
    "submission_rf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c69dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les prédictions pour la soumission\n",
    "xgb_pred_test = xgb_classifier.predict_proba(test_transformed)[:, 1]\n",
    "submission_xgb = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': xgb_pred_test})\n",
    "submission_xgb.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a723f4b",
   "metadata": {},
   "source": [
    "### 4. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31246bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de base pour LightGBM (conditionnel)\n",
    "if 'LGBM_AVAILABLE' in globals() and LGBM_AVAILABLE:\n",
    "    def get_lgbm_params():\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 100,\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 40,\n",
    "            'max_depth': -1,  # -1 signifie pas de limite\n",
    "            'subsample': 1.0,\n",
    "            'colsample_bytree': 1.0,\n",
    "            'reg_alpha': 0.0,\n",
    "            'reg_lambda': 0.0,\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 50\n",
    "        }\n",
    "        return params\n",
    "    \n",
    "    # Fonction d'évaluation avec arrêt précoce\n",
    "    def train_lgbm_model(X_train, y_train, X_val, y_val, params):\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        \n",
    "        # Configuration de l'arrêt précoce\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=10\n",
    "        )\n",
    "        \n",
    "        # Évaluation sur l'ensemble de validation\n",
    "        val_pred = model.predict_proba(X_val)[:, 1]\n",
    "        val_auc = roc_auc_score(y_val, val_pred)\n",
    "        print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        return model, val_auc\n",
    "    \n",
    "    # Diviser les données d'entraînement pour la validation\n",
    "    X_train_lgb, X_val, y_train_lgb, y_val = train_test_split(X_training_set, y_training_set, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Entraîner le modèle LightGBM\n",
    "    lgb_params = get_lgbm_params()\n",
    "    lgb_model, val_auc = train_lgbm_model(X_train_lgb, y_train_lgb, X_val, y_val, lgb_params)\n",
    "else:\n",
    "    print(\"LightGBM n'est pas disponible. Passage à l'étape suivante.\")\n",
    "    # Définir des valeurs vides\n",
    "    lgb_model = None\n",
    "    lgb_pred = np.zeros(len(y_validation_set))\n",
    "    lgb_pred_proba = np.zeros(len(y_validation_set))\n",
    "    lgb_new = np.zeros(len(new_test_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922801e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle LightGBM sur l'ensemble de validation\n",
    "lgb_pred = lgb_model.predict(X_validation_set)\n",
    "lgb_pred_proba = lgb_model.predict_proba(X_validation_set)[:, 1]\n",
    "\n",
    "print(\"LightGBM - Rapport de classification:\")\n",
    "print(classification_report(y_validation_set, lgb_pred))\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_validation_set, lgb_pred_proba):.4f}\")\n",
    "\n",
    "# Prédictions sur le nouveau jeu de test\n",
    "lgb_new = lgb_model.predict(new_test_transformed)\n",
    "print(\"\\nDistribution des prédictions sur le nouveau jeu de test:\")\n",
    "print(pd.Series(lgb_new).value_counts())\n",
    "\n",
    "# Visualiser l'importance des caractéristiques\n",
    "lgb_importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': lgb_model.feature_importances_ / sum(lgb_model.feature_importances_)\n",
    "})\n",
    "lgb_importance = plot_importance(lgb_model, features, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les prédictions pour la soumission\n",
    "lgb_pred_test = lgb_model.predict_proba(test_transformed)[:, 1]\n",
    "submission_lgb = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': lgb_pred_test})\n",
    "submission_lgb.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9585558b",
   "metadata": {},
   "source": [
    "### 5. Modèle d'ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f111db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un modèle d'ensemble par vote majoritaire\n",
    "def stacked_model(predictions):\n",
    "    \"\"\"Effectue un vote majoritaire sur les prédictions de plusieurs modèles.\"\"\"\n",
    "    stacked_predictions = np.array([])\n",
    "    \n",
    "    for element in predictions:\n",
    "        stacked_predictions = np.append(stacked_predictions, stats.mode(element)[0][0])\n",
    "        \n",
    "    return stacked_predictions\n",
    "\n",
    "# Combiner toutes les prédictions en un tableau multidimensionnel\n",
    "combined_array = np.column_stack([\n",
    "    log_regression_pred,\n",
    "    xgb_pred,\n",
    "    lgb_pred,\n",
    "    random_forest_pred\n",
    "])\n",
    "\n",
    "# Faire des prédictions avec le modèle par ensemble\n",
    "stacked_model_pred = stacked_model(combined_array)\n",
    "\n",
    "# Évaluer le modèle d'ensemble\n",
    "print(\"Modèle d'ensemble - Rapport de classification:\")\n",
    "print(classification_report(y_validation_set, stacked_model_pred))\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_validation_set, stacked_model_pred):.4f}\")\n",
    "\n",
    "# Prédictions sur le nouveau jeu de test\n",
    "combined_new = np.column_stack([\n",
    "    logistic_new,\n",
    "    xgb_new,\n",
    "    lgb_new,\n",
    "    random_forest_new\n",
    "])\n",
    "stacked_new = stacked_model(combined_new).astype(int)\n",
    "print(\"\\nDistribution des prédictions d'ensemble sur le nouveau jeu de test:\")\n",
    "print(pd.Series(stacked_new).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26fa519",
   "metadata": {},
   "source": [
    "## Sélection et sauvegarde du meilleur modèle\n",
    "\n",
    "Nous allons sauvegarder notre meilleur modèle au format .h5 pour l'utiliser ultérieurement dans l'application web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57df10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer tous les modèles disponibles sur l'ensemble de validation\n",
    "models = {\n",
    "    'Logistic Regression': {'model': logistic_regressor, 'auc': roc_auc_score(y_validation_set, log_regression_pred_proba)}\n",
    "}\n",
    "\n",
    "# Ajouter Random Forest si disponible\n",
    "if 'random_forest' in globals() and random_forest is not None:\n",
    "    models['Random Forest'] = {\n",
    "        'model': random_forest, \n",
    "        'auc': roc_auc_score(y_validation_set, random_forest_pred_proba)\n",
    "    }\n",
    "\n",
    "# Ajouter XGBoost si disponible\n",
    "if 'XGB_AVAILABLE' in globals() and XGB_AVAILABLE and xgb_classifier is not None:\n",
    "    models['XGBoost'] = {\n",
    "        'model': xgb_classifier, \n",
    "        'auc': roc_auc_score(y_validation_set, xgb_pred_proba)\n",
    "    }\n",
    "\n",
    "# Ajouter LightGBM si disponible\n",
    "if 'LGBM_AVAILABLE' in globals() and LGBM_AVAILABLE and lgb_model is not None:\n",
    "    models['LightGBM'] = {\n",
    "        'model': lgb_model, \n",
    "        'auc': roc_auc_score(y_validation_set, lgb_pred_proba)\n",
    "    }\n",
    "\n",
    "# Trouver le meilleur modèle\n",
    "best_model_name = max(models, key=lambda x: models[x]['auc'])\n",
    "best_model = models[best_model_name]['model']\n",
    "best_auc = models[best_model_name]['auc']\n",
    "\n",
    "print(f\"Le meilleur modèle est {best_model_name} avec un AUC-ROC de {best_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c10cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le répertoire models s'il n'existe pas\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Fonction pour sauvegarder différents types de modèles au format h5\n",
    "def save_model_as_h5(model, model_name):\n",
    "    \"\"\"\n",
    "    Sauvegarde un modèle au format h5 selon son type\n",
    "    \"\"\"\n",
    "    file_path = f'../models/{model_name}.h5'\n",
    "    \n",
    "    if 'LGBM_AVAILABLE' in globals() and LGBM_AVAILABLE and isinstance(model, lgb.LGBMModel):  # LightGBM models\n",
    "        model.booster_.save_model(file_path)\n",
    "        print(f\"Modèle LightGBM sauvegardé dans {file_path}\")\n",
    "        \n",
    "    elif 'XGB_AVAILABLE' in globals() and XGB_AVAILABLE and isinstance(model, XGBClassifier):  # XGBoost models\n",
    "        model.save_model(file_path)\n",
    "        print(f\"Modèle XGBoost sauvegardé dans {file_path}\")\n",
    "        \n",
    "    else:  # Scikit-learn models (RandomForest, LogisticRegression, etc.)\n",
    "        # Essayer de sauvegarder au format h5 si TensorFlow est disponible\n",
    "        if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "            from tensorflow.keras.models import Sequential\n",
    "            from tensorflow.keras.layers import Dense\n",
    "            \n",
    "            # Sauvegarder d'abord le modèle sklearn en utilisant pickle temporairement\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False).name\n",
    "            with open(temp_file, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "                \n",
    "            # Créer un modèle keras qui servira de conteneur\n",
    "            container_model = Sequential()\n",
    "            container_model.add(Dense(1, input_shape=(1,)))\n",
    "            \n",
    "            # Sauvegarder au format h5\n",
    "            container_model.save(file_path)\n",
    "            \n",
    "            # Ajouter le modèle sklearn comme attribut personnalisé\n",
    "            with h5py.File(file_path, 'a') as h5file:\n",
    "                sklearn_group = h5file.create_group('sklearn_model')\n",
    "                with open(temp_file, 'rb') as f:\n",
    "                    sklearn_group.create_dataset('model_dump', data=np.void(f.read()))\n",
    "                    \n",
    "            # Supprimer le fichier temporaire\n",
    "            os.remove(temp_file)\n",
    "            print(f\"Modèle {model.__class__.__name__} sauvegardé dans {file_path}\")\n",
    "        else:\n",
    "            # Utiliser pickle si TensorFlow n'est pas disponible\n",
    "            with open(file_path.replace('.h5', '.pkl'), 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            print(f\"TensorFlow n'est pas disponible. Modèle sauvegardé au format pickle: {file_path.replace('.h5', '.pkl')}\")\n",
    "    \n",
    "    # Sauvegarder également les métadonnées du modèle\n",
    "    metadata = {\n",
    "        'model_type': model.__class__.__name__,\n",
    "        'auc_score': models[best_model_name]['auc'],\n",
    "        'features': features\n",
    "    }\n",
    "    \n",
    "    with h5py.File(f'../models/{model_name}_metadata.h5', 'w') as h5file:\n",
    "        h5file.attrs['model_type'] = metadata['model_type']\n",
    "        h5file.attrs['auc_score'] = metadata['auc_score']\n",
    "        feature_group = h5file.create_group('features')\n",
    "        for i, feature in enumerate(features):\n",
    "            feature_group.attrs[f'feature_{i}'] = feature\n",
    "            \n",
    "    return file_path\n",
    "\n",
    "# Sauvegarder le meilleur modèle au format h5 ou pickle\n",
    "model_path = save_model_as_h5(best_model, best_model_name.lower().replace(' ', '_'))\n",
    "\n",
    "# Sauvegarder également l'imputer et le scaler pour une utilisation future\n",
    "with h5py.File('../models/preprocessing.h5', 'w') as h5file:\n",
    "    # Sauvegarder l'imputer et le scaler en tant qu'attributs personnalisés\n",
    "    imputer_temp = tempfile.NamedTemporaryFile(delete=False).name\n",
    "    scaler_temp = tempfile.NamedTemporaryFile(delete=False).name\n",
    "    \n",
    "    with open(imputer_temp, 'wb') as f:\n",
    "        pickle.dump(imputer, f)\n",
    "    with open(scaler_temp, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "        \n",
    "    # Créer des groupes pour l'imputer et le scaler\n",
    "    imputer_group = h5file.create_group('imputer')\n",
    "    scaler_group = h5file.create_group('scaler')\n",
    "    \n",
    "    # Ajouter les données sérialisées\n",
    "    with open(imputer_temp, 'rb') as f:\n",
    "        imputer_group.create_dataset('data', data=np.void(f.read()))\n",
    "    with open(scaler_temp, 'rb') as f:\n",
    "        scaler_group.create_dataset('data', data=np.void(f.read()))\n",
    "        \n",
    "    # Supprimer les fichiers temporaires\n",
    "    os.remove(imputer_temp)\n",
    "    os.remove(scaler_temp)\n",
    "    \n",
    "print(\"Préprocesseurs sauvegardés dans ../models/preprocessing.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefacf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour charger un modèle h5 selon son type\n",
    "def load_model_from_h5(file_path):\n",
    "    \"\"\"\n",
    "    Charge un modèle à partir d'un fichier h5\n",
    "    \"\"\"\n",
    "    # D'abord, déterminer le type de modèle à partir des métadonnées\n",
    "    metadata_path = file_path.replace('.h5', '_metadata.h5')\n",
    "    \n",
    "    with h5py.File(metadata_path, 'r') as h5file:\n",
    "        model_type = h5file.attrs['model_type']\n",
    "    \n",
    "    if model_type == 'LGBMClassifier':  # LightGBM models\n",
    "        model = lgb.Booster(model_file=file_path)\n",
    "        # Convertir le booster en LGBMClassifier\n",
    "        classifier = lgb.LGBMClassifier()\n",
    "        classifier._Booster = model\n",
    "        return classifier\n",
    "        \n",
    "    elif model_type == 'XGBClassifier':  # XGBoost models\n",
    "        model = XGBClassifier()\n",
    "        model.load_model(file_path)\n",
    "        return model\n",
    "        \n",
    "    else:  # Scikit-learn models (RandomForest, LogisticRegression, etc.)\n",
    "        # Charger le modèle depuis le format h5\n",
    "        with h5py.File(file_path, 'r') as h5file:\n",
    "            sklearn_group = h5file['sklearn_model']\n",
    "            model_dump = sklearn_group['model_dump'][()]\n",
    "            \n",
    "            # Écrire le dump dans un fichier temporaire\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False).name\n",
    "            with open(temp_file, 'wb') as f:\n",
    "                f.write(model_dump.tobytes())\n",
    "                \n",
    "            # Charger le modèle depuis le fichier temporaire\n",
    "            with open(temp_file, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "                \n",
    "            # Supprimer le fichier temporaire\n",
    "            os.remove(temp_file)\n",
    "            \n",
    "        return model\n",
    "\n",
    "# Test: chargement du modèle sauvegardé\n",
    "try:\n",
    "    loaded_model = load_model_from_h5(model_path)\n",
    "    print(f\"Modèle chargé avec succès : {type(loaded_model).__name__}\")\n",
    "    \n",
    "    # Vérifier que le modèle fonctionne correctement\n",
    "    sample_pred = loaded_model.predict_proba(X_validation_set[:5])[:, 1]\n",
    "    print(f\"Prédictions d'échantillon: {sample_pred}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement du modèle: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae2488",
   "metadata": {},
   "source": [
    "## Conclusion et prochaines étapes\n",
    "\n",
    "Dans ce notebook, nous avons:\n",
    "\n",
    "1. Exploré et préparé les données Home Credit pour la modélisation\n",
    "2. Créé de nouvelles caractéristiques via l'ingénierie des caractéristiques\n",
    "3. Entraîné et évalué plusieurs modèles de machine learning\n",
    "4. Comparé leurs performances et identifié le meilleur modèle\n",
    "\n",
    "### Observations clés\n",
    "\n",
    "- XGBoost et LightGBM ont généralement montré les meilleures performances\n",
    "- Les caractéristiques EXT_SOURCE sont parmi les plus importantes pour la prédiction\n",
    "- Notre modèle d'ensemble a amélioré légèrement les performances par rapport aux modèles individuels\n",
    "\n",
    "### Prochaines étapes\n",
    "\n",
    "1. **API REST** : Développer une API Flask pour servir notre meilleur modèle\n",
    "2. **Dashboard** : Créer un tableau de bord interactif avec Dash pour visualiser les prédictions et les explications\n",
    "3. **Optimisation** : Améliorer notre modèle en ajustant davantage les hyperparamètres et en intégrant plus de données\n",
    "4. **Interprétabilité** : Implémenter des méthodes comme SHAP pour expliquer les prédictions individuelles\n",
    "5. **Déploiement** : Mettre en place l'infrastructure pour déployer le modèle et le tableau de bord en production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35203ea5",
   "metadata": {},
   "source": [
    "### 2. Random Forest (Forêt aléatoire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94071718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n",
    "random_forest.fit(X_training_set, y_training_set)\n",
    "\n",
    "# Prédictions sur l'ensemble de validation\n",
    "random_forest_pred = random_forest.predict(X_validation_set)\n",
    "random_forest_pred_proba = random_forest.predict_proba(X_validation_set)[:, 1]\n",
    "\n",
    "# Évaluation du modèle\n",
    "print(\"Random Forest - Rapport de classification:\")\n",
    "print(classification_report(y_validation_set, random_forest_pred))\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_validation_set, random_forest_pred_proba):.4f}\")\n",
    "\n",
    "# Prédictions sur le nouveau jeu de test\n",
    "random_forest_new = random_forest.predict(new_test_transformed)\n",
    "print(\"\\nDistribution des prédictions sur le nouveau jeu de test:\")\n",
    "print(pd.Series(random_forest_new).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d2f1d",
   "metadata": {},
   "source": [
    "## 7.1 Comparaison des performances avec le modèle LightGBM\n",
    "\n",
    "Comparons les performances de notre meilleur modèle avec le modèle LightGBM de référence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
