{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb73bd5",
   "metadata": {},
   "source": [
    "# Exploration et Mod√©lisation pour le Score de Cr√©dit Home Credit\n",
    "\n",
    "Ce notebook pr√©sente l'exploration des donn√©es et la mod√©lisation pour le syst√®me de scoring de cr√©dit de Home Credit.\n",
    "\n",
    "**Note**: Ce projet est bas√© sur le mod√®le de [Home Credit Default Risk par rakshithvasudev](https://github.com/rakshithvasudev/Home-Credit-Default-Risk) que nous allons adapter et faire √©voluer pour notre cas d'utilisation.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "De nombreuses personnes ont des difficult√©s √† obtenir des pr√™ts en raison d'ant√©c√©dents de cr√©dit insuffisants ou inexistants. Malheureusement, cette population est souvent exploit√©e par des pr√™teurs peu scrupuleux.\n",
    "\n",
    "Home Credit s'efforce d'√©largir l'inclusion financi√®re pour la population non bancaris√©e en offrant une exp√©rience d'emprunt positive et s√ªre. Pour s'assurer que cette population mal desservie vive une exp√©rience de pr√™t positive, Home Credit utilise diverses donn√©es alternatives - y compris des informations sur les t√©l√©communications et les transactions - pour pr√©dire la capacit√© de remboursement de ses clients.\n",
    "\n",
    "L'objectif de ce projet est d'utiliser les donn√©es historiques des demandes de pr√™t pour pr√©dire si un candidat sera en mesure de rembourser un pr√™t ou non."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5dd62",
   "metadata": {},
   "source": [
    "## T√©l√©chargement des donn√©es depuis Kaggle\n",
    "\n",
    "Pour ce projet, nous utilisons le dataset de la comp√©tition Kaggle 'Home Credit Default Risk'. Voici comment t√©l√©charger et pr√©parer les donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faac77b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installation de l'API Kaggle si n√©cessaire\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7384b7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous devez configurer votre API Kaggle. Deux options:\n",
      "1. T√©l√©chargez kaggle.json depuis votre compte Kaggle (My Account > API) et placez-le dans ~/.kaggle/\n",
      "2. Ou entrez vos identifiants ci-dessous (ils ne seront pas affich√©s):\n",
      "Identifiants Kaggle sauvegard√©s avec succ√®s!\n",
      "Identifiants Kaggle sauvegard√©s avec succ√®s!\n"
     ]
    }
   ],
   "source": [
    "# Configuration des identifiants Kaggle\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Cr√©er le dossier Kaggle si n√©cessaire\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "\n",
    "# V√©rifier si le fichier d'API Kaggle existe d√©j√†\n",
    "kaggle_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
    "if not os.path.exists(kaggle_path):\n",
    "    print(\"Vous devez configurer votre API Kaggle. Deux options:\")\n",
    "    print(\"1. T√©l√©chargez kaggle.json depuis votre compte Kaggle (My Account > API) et placez-le dans ~/.kaggle/\")\n",
    "    print(\"2. Ou entrez vos identifiants ci-dessous (ils ne seront pas affich√©s):\")\n",
    "    \n",
    "    import getpass\n",
    "    username = input(\"Nom d'utilisateur Kaggle: \")\n",
    "    key = getpass.getpass(\"Cl√© API Kaggle: \")\n",
    "    \n",
    "    # Sauvegarder les identifiants\n",
    "    with open(kaggle_path, 'w') as f:\n",
    "        json.dump({\"username\": username, \"key\": key}, f)\n",
    "    \n",
    "    # Prot√©ger le fichier\n",
    "    os.chmod(kaggle_path, 0o600)\n",
    "    print(\"Identifiants Kaggle sauvegard√©s avec succ√®s!\")\n",
    "else:\n",
    "    print(\"Fichier d'API Kaggle trouv√©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5105154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement du dataset Home Credit Default Risk...\n",
      "401 Client Error: Unauthorized for url: https://www.kaggle.com/api/v1/competitions/data/download-all/home-credit-default-risk\n",
      "401 Client Error: Unauthorized for url: https://www.kaggle.com/api/v1/competitions/data/download-all/home-credit-default-risk\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/home-credit-default-risk.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Extraire les fichiers zip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhome-credit-default-risk.zip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m     14\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(DATA_PATH)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDonn√©es t√©l√©charg√©es et extraites avec succ√®s!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/zipfile/__init__.py:1331\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1331\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1333\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/home-credit-default-risk.zip'"
     ]
    }
   ],
   "source": [
    "# D√©finir le r√©pertoire de donn√©es et t√©l√©charger le dataset si n√©cessaire\n",
    "DATA_PATH = \"../data/\"\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "# V√©rifier si les donn√©es sont d√©j√† t√©l√©charg√©es\n",
    "if not os.path.exists(os.path.join(DATA_PATH, \"application_train.csv\")):\n",
    "    print(\"T√©l√©chargement du dataset Home Credit Default Risk...\")\n",
    "    # T√©l√©charger les donn√©es depuis Kaggle\n",
    "    !kaggle competitions download -c home-credit-default-risk -p {DATA_PATH}\n",
    "    \n",
    "    # Extraire les fichiers zip\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(os.path.join(DATA_PATH, 'home-credit-default-risk.zip'), 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_PATH)\n",
    "        \n",
    "    print(\"Donn√©es t√©l√©charg√©es et extraites avec succ√®s!\")\n",
    "else:\n",
    "    print(\"Les donn√©es sont d√©j√† t√©l√©charg√©es et extraites.\")\n",
    "\n",
    "# Afficher les fichiers de donn√©es disponibles\n",
    "print(\"\\nFichiers de donn√©es disponibles:\")\n",
    "for file in sorted(os.listdir(DATA_PATH)):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(DATA_PATH, file)\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\" - {file} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aper√ßu de la structure des donn√©es principales\n",
    "import pandas as pd\n",
    "\n",
    "# Charger les premi√®res lignes des fichiers cl√©s pour comprendre leur structure\n",
    "print(\"Structure du fichier application_train.csv:\")\n",
    "app_train = pd.read_csv(os.path.join(DATA_PATH, 'application_train.csv'), nrows=5)\n",
    "print(f\"Shape: {app_train.shape}\")\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b787552d",
   "metadata": {},
   "source": [
    "## Installation des d√©pendances sp√©ciales pour macOS\n",
    "\n",
    "XGBoost n√©cessite la biblioth√®que libomp sur macOS. Installons-la via Homebrew avant de continuer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd3bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©tection de macOS, installation de libomp pour XGBoost...\n",
      "/usr/local/bin/brew\n",
      "/usr/local/bin/brew\n",
      "Homebrew est install√©, installation de libomp...\n",
      "Homebrew est install√©, installation de libomp...\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updating Homebrew...\u001b[0m\n",
      "Adjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\n",
      "HOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updating Homebrew...\u001b[0m\n",
      "Adjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\n",
      "HOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/portable-ruby/portable-ruby/blobs/sha256:9fd394a40fb1467f89206a9c89c1274d9dc053af688176667a0cac0c3014113f\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/portable-ruby/portable-ruby/blobs/sha256:9fd394a40fb1467f89206a9c89c1274d9dc053af688176667a0cac0c3014113f\u001b[0m\n",
      "######################################################################### 100.0%###                                               38.6%###                                               38.6%############################        93.0%\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring portable-ruby-3.4.3.el_capitan.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring portable-ruby-3.4.3.el_capitan.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "Updated 4 taps (homebrew/services, mongodb/brew, symfony-cli/tap and homebrew/cask).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Casks\u001b[0m\n",
      "ace-studio                               font-source-han-code-jp\n",
      "advanced-renamer                         font-special-gothic\n",
      "agent-tars                               font-special-gothic-condensed-one\n",
      "antinote                                 font-special-gothic-expanded-one\n",
      "aqua-voice                               font-wdxl-lubrifont-jp-n\n",
      "atv-remote                               font-wdxl-lubrifont-sc\n",
      "automounterhelper                        font-wdxl-lubrifont-tc\n",
      "bambu-connect                            font-webdings\n",
      "batfi                                    font-winky-rough\n",
      "beutl                                    font-winky-sans\n",
      "block-goose                              fuse-t\n",
      "candy-crisis                             gologin\n",
      "captainplugins                           granola\n",
      "charles@4                                grayjay\n",
      "chime@alpha                              hamrs-pro\n",
      "chordpotion                              hedy\n",
      "cloudflare-warp@beta                     highlight\n",
      "cloudpouch                               hy-rpe2\n",
      "clover-chord-systems                     ijhttp\n",
      "comfyui                                  inmusic-software-center\n",
      "companion                                irpf2025\n",
      "companion-satellite                      istatistica-core\n",
      "companion@beta                           jumpcloud-password-manager\n",
      "consul                                   k6-studio\n",
      "coterm                                   kate\n",
      "dante-controller                         kilohearts-installer\n",
      "dante-via                                kunkun\n",
      "deepchat                                 liviable\n",
      "desktime                                 losslessswitcher\n",
      "earnapp                                  luanti\n",
      "elemental                                macskk\n",
      "elemental@6                              meru\n",
      "excire-foto                              mitti\n",
      "focu                                     moment\n",
      "foks                                     mouseless@preview\n",
      "font-adwaita                             ndi-tools\n",
      "font-adwaita-mono-nerd-font              nethlink\n",
      "font-ancizar-sans                        notion-mail\n",
      "font-ancizar-serif                       nvidia-nsight-compute\n",
      "font-aporetic                            obscura-vpn\n",
      "font-asta-sans                           opencloud\n",
      "font-atkynson-mono-nerd-font             opera-air\n",
      "font-big-shoulders                       outerbase-studio\n",
      "font-big-shoulders-inline                ovice\n",
      "font-big-shoulders-stencil               pairpods\n",
      "font-bizter                              pareto-security\n",
      "font-boldonse                            pastenow\n",
      "font-bytesized                           pdl\n",
      "font-comic-relief                        precize\n",
      "font-coral-pixels                        profit\n",
      "font-epunda-sans                         qobuz-downloader\n",
      "font-epunda-slab                         qt-design-studio\n",
      "font-exile                               rave\n",
      "font-fzhei-b01                           realvnc-connect\n",
      "font-fzxiheii-z08                        repo-prompt\n",
      "font-harmonyos-sans                      restapia\n",
      "font-harmonyos-sans-naskh-arabic         sc-menu\n",
      "font-harmonyos-sans-sc                   slidepad\n",
      "font-harmonyos-sans-tc                   sokim\n",
      "font-huninn                              soundanchor\n",
      "font-jetbrains-maple-mono                stability-matrix\n",
      "font-jetbrains-maple-mono-nf             swift-shift\n",
      "font-kumar-one-outline                   tal-drum\n",
      "font-libertinus-math                     thelowtechguys-cling\n",
      "font-libertinus-mono                     trae\n",
      "font-lxgw-wenkai-gb-lite                 trae-cn\n",
      "font-m-plus-rounded-1c                   triliumnext-notes\n",
      "font-maple-mono-normal                   ua-midi-control\n",
      "font-maple-mono-normal-cn                veracrypt-fuse-t\n",
      "font-maple-mono-normal-nf                vesktop\n",
      "font-maple-mono-normal-nf-cn             vezer\n",
      "font-noto-serif-dives-akuru              viables\n",
      "font-playpen-sans-arabic                 vimy\n",
      "font-playpen-sans-deva                   voiceink\n",
      "font-playpen-sans-hebrew                 warp@preview\n",
      "font-playpen-sans-thai                   windsurf@next\n",
      "font-pretendard-gov                      witsy\n",
      "font-sf-mono-nerd-font-ligaturized       yaak@beta\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "Updated 4 taps (homebrew/services, mongodb/brew, symfony-cli/tap and homebrew/cask).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Casks\u001b[0m\n",
      "ace-studio                               font-source-han-code-jp\n",
      "advanced-renamer                         font-special-gothic\n",
      "agent-tars                               font-special-gothic-condensed-one\n",
      "antinote                                 font-special-gothic-expanded-one\n",
      "aqua-voice                               font-wdxl-lubrifont-jp-n\n",
      "atv-remote                               font-wdxl-lubrifont-sc\n",
      "automounterhelper                        font-wdxl-lubrifont-tc\n",
      "bambu-connect                            font-webdings\n",
      "batfi                                    font-winky-rough\n",
      "beutl                                    font-winky-sans\n",
      "block-goose                              fuse-t\n",
      "candy-crisis                             gologin\n",
      "captainplugins                           granola\n",
      "charles@4                                grayjay\n",
      "chime@alpha                              hamrs-pro\n",
      "chordpotion                              hedy\n",
      "cloudflare-warp@beta                     highlight\n",
      "cloudpouch                               hy-rpe2\n",
      "clover-chord-systems                     ijhttp\n",
      "comfyui                                  inmusic-software-center\n",
      "companion                                irpf2025\n",
      "companion-satellite                      istatistica-core\n",
      "companion@beta                           jumpcloud-password-manager\n",
      "consul                                   k6-studio\n",
      "coterm                                   kate\n",
      "dante-controller                         kilohearts-installer\n",
      "dante-via                                kunkun\n",
      "deepchat                                 liviable\n",
      "desktime                                 losslessswitcher\n",
      "earnapp                                  luanti\n",
      "elemental                                macskk\n",
      "elemental@6                              meru\n",
      "excire-foto                              mitti\n",
      "focu                                     moment\n",
      "foks                                     mouseless@preview\n",
      "font-adwaita                             ndi-tools\n",
      "font-adwaita-mono-nerd-font              nethlink\n",
      "font-ancizar-sans                        notion-mail\n",
      "font-ancizar-serif                       nvidia-nsight-compute\n",
      "font-aporetic                            obscura-vpn\n",
      "font-asta-sans                           opencloud\n",
      "font-atkynson-mono-nerd-font             opera-air\n",
      "font-big-shoulders                       outerbase-studio\n",
      "font-big-shoulders-inline                ovice\n",
      "font-big-shoulders-stencil               pairpods\n",
      "font-bizter                              pareto-security\n",
      "font-boldonse                            pastenow\n",
      "font-bytesized                           pdl\n",
      "font-comic-relief                        precize\n",
      "font-coral-pixels                        profit\n",
      "font-epunda-sans                         qobuz-downloader\n",
      "font-epunda-slab                         qt-design-studio\n",
      "font-exile                               rave\n",
      "font-fzhei-b01                           realvnc-connect\n",
      "font-fzxiheii-z08                        repo-prompt\n",
      "font-harmonyos-sans                      restapia\n",
      "font-harmonyos-sans-naskh-arabic         sc-menu\n",
      "font-harmonyos-sans-sc                   slidepad\n",
      "font-harmonyos-sans-tc                   sokim\n",
      "font-huninn                              soundanchor\n",
      "font-jetbrains-maple-mono                stability-matrix\n",
      "font-jetbrains-maple-mono-nf             swift-shift\n",
      "font-kumar-one-outline                   tal-drum\n",
      "font-libertinus-math                     thelowtechguys-cling\n",
      "font-libertinus-mono                     trae\n",
      "font-lxgw-wenkai-gb-lite                 trae-cn\n",
      "font-m-plus-rounded-1c                   triliumnext-notes\n",
      "font-maple-mono-normal                   ua-midi-control\n",
      "font-maple-mono-normal-cn                veracrypt-fuse-t\n",
      "font-maple-mono-normal-nf                vesktop\n",
      "font-maple-mono-normal-nf-cn             vezer\n",
      "font-noto-serif-dives-akuru              viables\n",
      "font-playpen-sans-arabic                 vimy\n",
      "font-playpen-sans-deva                   voiceink\n",
      "font-playpen-sans-hebrew                 warp@preview\n",
      "font-playpen-sans-thai                   windsurf@next\n",
      "font-pretendard-gov                      witsy\n",
      "font-sf-mono-nerd-font-ligaturized       yaak@beta\n",
      "\n",
      "You have \u001b[1m85\u001b[0m outdated formulae installed.\n",
      "\n",
      "\n",
      "You have \u001b[1m85\u001b[0m outdated formulae installed.\n",
      "\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libomp/manifests/20.1.4\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libomp/manifests/20.1.4\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mlibomp\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libomp/blobs/sha256:9a78864f40e\u001b[0m\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mlibomp\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libomp/blobs/sha256:9a78864f40e\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libomp--20.1.4.sonoma.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring libomp--20.1.4.sonoma.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mCaveats\u001b[0m\n",
      "libomp is keg-only, which means it was not symlinked into /usr/local,\n",
      "because it can override GCC headers and result in broken builds.\n",
      "\n",
      "For compilers to find libomp you may need to set:\n",
      "  export LDFLAGS=\"-L/usr/local/opt/libomp/lib\"\n",
      "  export CPPFLAGS=\"-I/usr/local/opt/libomp/include\"\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSummary\u001b[0m\n",
      "üç∫  /usr/local/Cellar/libomp/20.1.4: 9 files, 1.7MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup libomp`...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mCaveats\u001b[0m\n",
      "libomp is keg-only, which means it was not symlinked into /usr/local,\n",
      "because it can override GCC headers and result in broken builds.\n",
      "\n",
      "For compilers to find libomp you may need to set:\n",
      "  export LDFLAGS=\"-L/usr/local/opt/libomp/lib\"\n",
      "  export CPPFLAGS=\"-I/usr/local/opt/libomp/include\"\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSummary\u001b[0m\n",
      "üç∫  /usr/local/Cellar/libomp/20.1.4: 9 files, 1.7MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup libomp`...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n"
     ]
    }
   ],
   "source": [
    "# V√©rifier si nous sommes sur macOS et installer libomp si n√©cessaire\n",
    "import platform\n",
    "import os\n",
    "\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    print(\"D√©tection de macOS, installation de libomp pour XGBoost...\")\n",
    "    # V√©rifier si Homebrew est install√©\n",
    "    try:\n",
    "        !which brew\n",
    "        print(\"Homebrew est install√©, installation de libomp...\")\n",
    "        !brew install libomp\n",
    "    except:\n",
    "        print(\"Homebrew n'est pas install√©. Veuillez installer Homebrew puis libomp:\")\n",
    "        print(\"1. /bin/bash -c \\\"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\\\"\")\n",
    "        print(\"2. brew install libomp\")\n",
    "else:\n",
    "    print(f\"Syst√®me d'exploitation d√©tect√©: {platform.system()}. Pas besoin d'installation sp√©ciale pour XGBoost.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9a2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation de XGBoost avec les param√®tres sp√©cifiques pour macOS...\n",
      "Requirement already satisfied: xgboost in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from xgboost) (1.14.0)\n",
      "Requirement already satisfied: xgboost in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from xgboost) (1.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: lightgbm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.11.0)\n",
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: keras in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lightgbm) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.3.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.65.4)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: lightgbm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.11.0)\n",
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: keras in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lightgbm) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.3.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.65.4)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installation des biblioth√®ques n√©cessaires avec options sp√©cifiques pour macOS\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    print(\"Installation de XGBoost avec les param√®tres sp√©cifiques pour macOS...\")\n",
    "    !pip install --no-binary xgboost xgboost\n",
    "    !pip install lightgbm h5py tensorflow keras\n",
    "else:\n",
    "    !pip install xgboost lightgbm h5py tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801062fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avertissement: imbalanced-learn n'est pas disponible. SMOTE sera d√©sactiv√©.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 09:33:58.687074: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import des biblioth√®ques n√©cessaires\n",
    "import numpy as np  # pour les calculs math√©matiques\n",
    "import pandas as pd  # pour la manipulation des donn√©es (csv)\n",
    "import matplotlib.pyplot as plt  # pour les graphiques\n",
    "import seaborn as sns  # pour plus d'options de graphiques (construit sur matplotlib)\n",
    "\n",
    "# Imports pour le machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer  # Remplace Imputer qui est obsol√®te\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Tenter d'importer XGBoost avec gestion d'erreurs\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"Avertissement: XGBoost n'a pas pu √™tre import√©: {e}\")\n",
    "    print(\"Les fonctionnalit√©s utilisant XGBoost seront d√©sactiv√©es.\")\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "# Tenter d'importer LightGBM avec gestion d'erreurs\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGBM_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"Avertissement: LightGBM n'a pas pu √™tre import√©: {e}\")\n",
    "    print(\"Les fonctionnalit√©s utilisant LightGBM seront d√©sactiv√©es.\")\n",
    "    LGBM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Avertissement: imbalanced-learn n'est pas disponible. SMOTE sera d√©sactiv√©.\")\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Pour la sauvegarde de mod√®le au format h5\n",
    "import h5py\n",
    "import tempfile\n",
    "\n",
    "try:\n",
    "    from tensorflow import keras\n",
    "    import joblib\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Avertissement: TensorFlow/Keras n'est pas disponible. Le format h5 sera limit√©.\")\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "# Supprimer les avertissements inutiles pour que la pr√©sentation soit claire\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Afficher les graphiques dans le notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration pour afficher plus de colonnes\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53684a1",
   "metadata": {},
   "source": [
    "## Chargement des donn√©es\n",
    "\n",
    "Dans cette section, nous allons charger les donn√©es Home Credit. Dans un environnement de production, nous chargerions les fichiers de donn√©es r√©els."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a8172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentative de chargement des donn√©es r√©elles de Home Credit...\n",
      "Erreur lors du chargement des donn√©es r√©elles: [Errno 2] No such file or directory: '../data/application_train.csv'\n",
      "Utilisation des donn√©es simul√©es pour le moment...\n"
     ]
    }
   ],
   "source": [
    "# D√©finition des chemins de fichiers\n",
    "DATA_PATH = \"../data/\"\n",
    "\n",
    "# Tenter de charger les donn√©es r√©elles depuis votre dossier local\n",
    "try:\n",
    "    print(\"Chargement des donn√©es locales depuis le dossier data...\")\n",
    "    train = pd.read_csv(os.path.join(DATA_PATH, 'application_train.csv'))\n",
    "    print(f\"Donn√©es d'entra√Ænement charg√©es avec succ√®s - {train.shape[0]} lignes et {train.shape[1]} colonnes\")\n",
    "    \n",
    "    if 'application_test.csv' in available_files:\n",
    "        test = pd.read_csv(os.path.join(DATA_PATH, 'application_test.csv'))\n",
    "        print(f\"Donn√©es de test charg√©es avec succ√®s - {test.shape[0]} lignes et {test.shape[1]} colonnes\")\n",
    "    else:\n",
    "        # Cr√©er un jeu de test √† partir du jeu d'entra√Ænement si le fichier test n'est pas disponible\n",
    "        print(\"Fichier application_test.csv non trouv√©, cr√©ation d'un √©chantillon de test √† partir des donn√©es d'entra√Ænement\")\n",
    "        # S√©parer 20% des donn√©es d'entra√Ænement pour le test\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train_data, test_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "        train = train_data.reset_index(drop=True)\n",
    "        test = test_data.drop('TARGET', axis=1).reset_index(drop=True)\n",
    "        print(f\"Jeu de test cr√©√© avec {test.shape[0]} lignes\")\n",
    "    \n",
    "    # Cr√©er un petit √©chantillon du jeu de test pour les pr√©dictions rapides\n",
    "    new_test = test.iloc[:100].copy()\n",
    "    print(f\"√âchantillon de test pour nouvelles pr√©dictions: {new_test.shape}\")\n",
    "    \n",
    "    print(\"\\nDonn√©es charg√©es avec succ√®s!\")\n",
    "    DATA_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement des donn√©es: {e}\")\n",
    "    print(\"Utilisation des donn√©es simul√©es pour le moment...\")\n",
    "    DATA_AVAILABLE = False\n",
    "    \n",
    "    # Code pour g√©n√©rer des donn√©es simul√©es (inchang√©)\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Identifiants clients\n",
    "    client_ids = np.arange(n_samples)\n",
    "    \n",
    "    # Caract√©ristiques des pr√™ts\n",
    "    loan_amounts = np.random.normal(15000, 5000, n_samples)\n",
    "    loan_terms = np.random.choice([12, 24, 36, 48, 60], n_samples)\n",
    "    interest_rates = np.random.uniform(0.03, 0.15, n_samples)\n",
    "    \n",
    "    # Caract√©ristiques des clients\n",
    "    ages = np.random.normal(40, 10, n_samples).astype(int)\n",
    "    incomes = np.random.normal(50000, 15000, n_samples)\n",
    "    employment_years = np.random.normal(7, 4, n_samples)\n",
    "    debt_to_income = np.random.normal(0.3, 0.1, n_samples)\n",
    "    credit_scores = np.random.normal(650, 100, n_samples).astype(int)\n",
    "    \n",
    "    # Historique de pr√™ts\n",
    "    num_previous_loans = np.random.poisson(2, n_samples)\n",
    "    num_delinquencies = np.random.poisson(0.5, n_samples)\n",
    "    \n",
    "    # Cr√©ation de la variable cible (d√©faut de paiement)\n",
    "    probabilities = 1.0 / (1.0 + np.exp(-(0.0001 * loan_amounts - 0.05 * employment_years \n",
    "                                      + 0.1 * debt_to_income - 0.0001 * credit_scores \n",
    "                                      + 0.2 * num_delinquencies - 3)))\n",
    "    target = np.random.binomial(1, probabilities)\n",
    "    \n",
    "    # Cr√©ation du DataFrame\n",
    "    train = pd.DataFrame({\n",
    "        'SK_ID_CURR': client_ids,\n",
    "        'TARGET': target,\n",
    "        'NAME_CONTRACT_TYPE': np.random.choice(['Cash loans', 'Revolving loans'], n_samples),\n",
    "        'CODE_GENDER': np.random.choice(['M', 'F'], n_samples),\n",
    "        'FLAG_OWN_CAR': np.random.choice(['Y', 'N'], n_samples),\n",
    "        'FLAG_OWN_REALTY': np.random.choice(['Y', 'N'], n_samples),\n",
    "        'CNT_CHILDREN': np.random.poisson(0.5, n_samples),\n",
    "        'AMT_INCOME_TOTAL': incomes,\n",
    "        'AMT_CREDIT': loan_amounts,\n",
    "        'AMT_ANNUITY': loan_amounts * (interest_rates / 12) * (1 + interest_rates / 12) ** loan_terms / ((1 + interest_rates / 12) ** loan_terms - 1),\n",
    "        'AMT_GOODS_PRICE': loan_amounts * 0.9,\n",
    "        'DAYS_BIRTH': -ages * 365,  # Convertir l'√¢ge en jours n√©gatifs\n",
    "        'DAYS_EMPLOYED': -employment_years * 365,  # Convertir les ann√©es en jours n√©gatifs\n",
    "        'REGION_POPULATION_RELATIVE': np.random.uniform(0.001, 0.07, n_samples),\n",
    "        'EXT_SOURCE_1': np.random.uniform(0, 1, n_samples),\n",
    "        'EXT_SOURCE_2': np.random.uniform(0, 1, n_samples),\n",
    "        'EXT_SOURCE_3': np.random.uniform(0, 1, n_samples),\n",
    "    })\n",
    "    \n",
    "    # Cr√©ation d'un jeu de test qui ressemble au jeu d'entra√Ænement mais sans la colonne TARGET\n",
    "    test = train.copy().drop('TARGET', axis=1).iloc[:int(n_samples/5)]\n",
    "    \n",
    "    # Cr√©ation d'un jeu de test 'new_test' pour les pr√©dictions\n",
    "    new_test = train.copy().drop('TARGET', axis=1).iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour r√©duire l'utilisation de la m√©moire\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" R√©duit l'usage m√©moire d'un DataFrame en convertissant les types de donn√©es \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('M√©moire utilis√©e par le DataFrame: {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('M√©moire utilis√©e apr√®s optimisation: {:.2f} MB'.format(end_mem))\n",
    "    print('R√©duction de {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Optimiser l'utilisation de la m√©moire si les donn√©es sont disponibles\n",
    "if DATA_AVAILABLE:\n",
    "    print(\"Optimisation de l'utilisation de la m√©moire...\")\n",
    "    print(\"\\nOptimisation du jeu d'entra√Ænement:\")\n",
    "    train = reduce_mem_usage(train)\n",
    "    print(\"\\nOptimisation du jeu de test:\")\n",
    "    test = reduce_mem_usage(test)\n",
    "    print(\"\\nOptimisation de l'√©chantillon de test:\")\n",
    "    new_test = reduce_mem_usage(new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6237369",
   "metadata": {},
   "source": [
    "## Exploration des donn√©es\n",
    "\n",
    "Examinons les donn√©es pour comprendre leur structure et leurs caract√©ristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10a100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du jeu d'entra√Ænement: (1000, 17)\n",
      "Dimensions du jeu de test: (200, 16)\n",
      "Dimensions du nouveau jeu de test: (100, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>40394.034814</td>\n",
       "      <td>17483.570765</td>\n",
       "      <td>395.709049</td>\n",
       "      <td>15735.213689</td>\n",
       "      <td>-17155</td>\n",
       "      <td>-2262.127190</td>\n",
       "      <td>0.059961</td>\n",
       "      <td>0.460813</td>\n",
       "      <td>0.783607</td>\n",
       "      <td>0.688727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>37740.568087</td>\n",
       "      <td>14308.678494</td>\n",
       "      <td>1246.627856</td>\n",
       "      <td>12877.810645</td>\n",
       "      <td>-14600</td>\n",
       "      <td>-3336.165980</td>\n",
       "      <td>0.039912</td>\n",
       "      <td>0.478720</td>\n",
       "      <td>0.197452</td>\n",
       "      <td>0.951414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>38521.664510</td>\n",
       "      <td>18238.442691</td>\n",
       "      <td>549.471041</td>\n",
       "      <td>16414.598421</td>\n",
       "      <td>-17155</td>\n",
       "      <td>-4071.847672</td>\n",
       "      <td>0.045788</td>\n",
       "      <td>0.177668</td>\n",
       "      <td>0.558449</td>\n",
       "      <td>0.562367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>50094.267873</td>\n",
       "      <td>22615.149282</td>\n",
       "      <td>429.273935</td>\n",
       "      <td>20353.634354</td>\n",
       "      <td>-11315</td>\n",
       "      <td>-3329.533758</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.876208</td>\n",
       "      <td>0.113097</td>\n",
       "      <td>0.960756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>13487.354212</td>\n",
       "      <td>13829.233126</td>\n",
       "      <td>409.084647</td>\n",
       "      <td>12446.309814</td>\n",
       "      <td>-19710</td>\n",
       "      <td>-3775.298760</td>\n",
       "      <td>0.026665</td>\n",
       "      <td>0.525821</td>\n",
       "      <td>0.361435</td>\n",
       "      <td>0.892455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0           0       1    Revolving loans           M            Y   \n",
       "1           1       0    Revolving loans           F            Y   \n",
       "2           2       0    Revolving loans           F            Y   \n",
       "3           3       0         Cash loans           F            Y   \n",
       "4           4       0    Revolving loans           F            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL    AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             1      40394.034814  17483.570765   395.709049   \n",
       "1               N             2      37740.568087  14308.678494  1246.627856   \n",
       "2               Y             0      38521.664510  18238.442691   549.471041   \n",
       "3               N             1      50094.267873  22615.149282   429.273935   \n",
       "4               Y             1      13487.354212  13829.233126   409.084647   \n",
       "\n",
       "   AMT_GOODS_PRICE  DAYS_BIRTH  DAYS_EMPLOYED  REGION_POPULATION_RELATIVE  \\\n",
       "0     15735.213689      -17155   -2262.127190                    0.059961   \n",
       "1     12877.810645      -14600   -3336.165980                    0.039912   \n",
       "2     16414.598421      -17155   -4071.847672                    0.045788   \n",
       "3     20353.634354      -11315   -3329.533758                    0.022200   \n",
       "4     12446.309814      -19710   -3775.298760                    0.026665   \n",
       "\n",
       "   EXT_SOURCE_1  EXT_SOURCE_2  EXT_SOURCE_3  \n",
       "0      0.460813      0.783607      0.688727  \n",
       "1      0.478720      0.197452      0.951414  \n",
       "2      0.177668      0.558449      0.562367  \n",
       "3      0.876208      0.113097      0.960756  \n",
       "4      0.525821      0.361435      0.892455  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examiner les dimensions des jeux de donn√©es\n",
    "print(\"Dimensions du jeu d'entra√Ænement:\", train.shape)\n",
    "print(\"Dimensions du jeu de test:\", test.shape)\n",
    "print(\"Dimensions du nouveau jeu de test:\", new_test.shape)\n",
    "\n",
    "# Afficher les premi√®res lignes du jeu d'entra√Ænement\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e721ac02",
   "metadata": {},
   "source": [
    "## 1. Chargement des donn√©es\n",
    "\n",
    "Dans cette section, nous allons charger les donn√©es de Home Credit. Les donn√©es sont dispers√©es dans plusieurs tables qu'il faudra joindre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d21aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes avec valeurs manquantes dans le jeu d'entra√Ænement:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Count</th>\n",
       "      <th>Missing Count Ratio</th>\n",
       "      <th>Missing Count %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Count, Missing Count Ratio, Missing Count %]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction pour trouver les colonnes avec des valeurs manquantes\n",
    "def missing_columns(dataframe):\n",
    "    \"\"\"\n",
    "    Renvoie un dataframe contenant les noms des colonnes manquantes et \n",
    "    le pourcentage de valeurs manquantes par rapport √† l'ensemble du dataframe.\n",
    "    \n",
    "    dataframe: dataframe qui donne les noms des colonnes et leur % de valeurs manquantes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Trouver les valeurs manquantes\n",
    "    missing_values = dataframe.isnull().sum().sort_values(ascending=False)\n",
    "    \n",
    "    # Pourcentage de valeurs manquantes par rapport √† la taille totale\n",
    "    missing_values_pct = 100 * missing_values/len(dataframe)\n",
    "    \n",
    "    # Cr√©er un nouveau dataframe qui est une version concat√©n√©e\n",
    "    concat_values = pd.concat([missing_values, missing_values/len(dataframe), missing_values_pct.round(1)], axis=1)\n",
    "\n",
    "    # Donner de nouveaux noms de colonne\n",
    "    concat_values.columns = ['Missing Count', 'Missing Count Ratio', 'Missing Count %']\n",
    "    \n",
    "    # Retourner les valeurs requises\n",
    "    return concat_values[concat_values.iloc[:,1]!=0]\n",
    "    \n",
    "# Afficher les colonnes avec des valeurs manquantes\n",
    "print(\"Colonnes avec valeurs manquantes dans le jeu d'entra√Ænement:\")\n",
    "missing_columns(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98b761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes avec valeurs manquantes dans le jeu d'entra√Ænement:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Count</th>\n",
       "      <th>Missing Count Ratio</th>\n",
       "      <th>Missing Count %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Count, Missing Count Ratio, Missing Count %]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction pour trouver les colonnes avec des valeurs manquantes\n",
    "def missing_columns(dataframe):\n",
    "    \"\"\"\n",
    "    Renvoie un dataframe contenant les noms des colonnes manquantes et \n",
    "    le pourcentage de valeurs manquantes par rapport √† l'ensemble du dataframe.\n",
    "    \n",
    "    dataframe: dataframe qui donne les noms des colonnes et leur % de valeurs manquantes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Trouver les valeurs manquantes\n",
    "    missing_values = dataframe.isnull().sum().sort_values(ascending=False)\n",
    "    \n",
    "    # Pourcentage de valeurs manquantes par rapport √† la taille totale\n",
    "    missing_values_pct = 100 * missing_values/len(dataframe)\n",
    "    \n",
    "    # Cr√©er un nouveau dataframe qui est une version concat√©n√©e\n",
    "    concat_values = pd.concat([missing_values, missing_values/len(dataframe), missing_values_pct.round(1)], axis=1)\n",
    "\n",
    "    # Donner de nouveaux noms de colonne\n",
    "    concat_values.columns = ['Missing Count', 'Missing Count Ratio', 'Missing Count %']\n",
    "    \n",
    "    # Retourner les valeurs requises\n",
    "    return concat_values[concat_values.iloc[:,1]!=0]\n",
    "    \n",
    "# Afficher les colonnes avec des valeurs manquantes\n",
    "print(\"Colonnes avec valeurs manquantes dans le jeu d'entra√Ænement:\")\n",
    "missing_columns(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e98d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean       39.553000\n",
       "std        10.095995\n",
       "min         7.000000\n",
       "25%        33.000000\n",
       "50%        40.000000\n",
       "75%        46.000000\n",
       "max        72.000000\n",
       "Name: DAYS_BIRTH, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explorer l'√¢ge des clients\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.distplot(train['DAYS_BIRTH'] / -365, bins=25, kde=False)\n",
    "plt.xlabel(\"√Çge du client (ann√©es)\")\n",
    "plt.title('Distribution des √¢ges')\n",
    "plt.show()\n",
    "\n",
    "# Statistiques de l'√¢ge\n",
    "(train['DAYS_BIRTH'] / -365).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0876f50",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Cr√©ons de nouvelles caract√©ristiques qui pourraient √™tre utiles pour la pr√©diction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab703be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'anomalies avec 1000 ans d'emploi: 0\n",
      "Pas assez d'anomalies pour calculer le taux de d√©faut\n",
      "Non-anomalies ont un taux de d√©faut de 15.90%\n"
     ]
    }
   ],
   "source": [
    "# V√©rifier les anomalies dans DAYS_EMPLOYED\n",
    "thousand_anomalies = train[(train['DAYS_EMPLOYED']/365 >= 900) & (train['DAYS_EMPLOYED']/365 <= 1100)]\n",
    "print(f\"Nombre d'anomalies avec 1000 ans d'emploi: {len(thousand_anomalies)}\")\n",
    "\n",
    "# Comparer le taux de d√©faut entre les anomalies et les non-anomalies\n",
    "anomalies_index = pd.Index(thousand_anomalies.index)\n",
    "non_anomalies_index = train.index.difference(anomalies_index)\n",
    "non_anomalies = train.iloc[non_anomalies_index]\n",
    "\n",
    "anomalies_target = thousand_anomalies['TARGET'].value_counts()\n",
    "non_anomalies_target = non_anomalies['TARGET'].value_counts()\n",
    "\n",
    "if len(anomalies_target) > 1 and 1 in anomalies_target and 0 in anomalies_target:\n",
    "    print(f\"Anomalies ont un taux de d√©faut de {100*anomalies_target[1]/(anomalies_target[1]+anomalies_target[0]):.2f}%\")\n",
    "else:\n",
    "    print(\"Pas assez d'anomalies pour calculer le taux de d√©faut\")\n",
    "    \n",
    "if len(non_anomalies_target) > 1 and 1 in non_anomalies_target and 0 in non_anomalies_target:\n",
    "    print(f\"Non-anomalies ont un taux de d√©faut de {100*non_anomalies_target[1]/(non_anomalies_target[1]+non_anomalies_target[0]):.2f}%\")\n",
    "else:\n",
    "    print(\"Pas assez de non-anomalies pour calculer le taux de d√©faut\")\n",
    "\n",
    "# Cr√©er une colonne indicatrice d'anomalie et remplacer les valeurs anomaliques par NaN\n",
    "train['DAYS_EMPLOYED_ANOM'] = train[\"DAYS_EMPLOYED\"] == 365243\n",
    "train['DAYS_EMPLOYED'] = train['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "\n",
    "# Faire de m√™me pour les jeux de test\n",
    "test['DAYS_EMPLOYED_ANOM'] = test[\"DAYS_EMPLOYED\"] == 365243\n",
    "test['DAYS_EMPLOYED'] = test['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "\n",
    "new_test['DAYS_EMPLOYED_ANOM'] = new_test[\"DAYS_EMPLOYED\"] == 365243\n",
    "new_test['DAYS_EMPLOYED'] = new_test['DAYS_EMPLOYED'].replace({365243: np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965fb2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Revolving loans'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Explorer les corr√©lations avec la variable cible\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m corr_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTARGET\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msort_values()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Afficher les 10 variables les plus n√©gativement corr√©l√©es\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariables les plus n√©gativement corr√©l√©es √† TARGET:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:11049\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[0;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[1;32m  11047\u001b[0m cols \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m  11048\u001b[0m idx \u001b[38;5;241m=\u001b[39m cols\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m> 11049\u001b[0m mat \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m  11051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m  11052\u001b[0m     correl \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mnancorr(mat, minp\u001b[38;5;241m=\u001b[39mmin_periods)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:1993\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1992\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[0;32m-> 1993\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[1;32m   1995\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(result, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/managers.py:1694\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1692\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1694\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/managers.py:1753\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[0;34m(self, dtype, na_value)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m         arr \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mget_values(dtype)\n\u001b[0;32m-> 1753\u001b[0m     \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m arr\n\u001b[1;32m   1754\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Revolving loans'"
     ]
    }
   ],
   "source": [
    "# Explorer les corr√©lations avec la variable cible\n",
    "# Utiliser numeric_only=True pour √©viter l'erreur avec les colonnes non num√©riques\n",
    "corr_train = train.corr(numeric_only=True)['TARGET'].sort_values()\n",
    "\n",
    "# Afficher les 10 variables les plus n√©gativement corr√©l√©es\n",
    "print(\"Variables les plus n√©gativement corr√©l√©es √† TARGET:\")\n",
    "print(corr_train.head(10))\n",
    "\n",
    "# Afficher les 10 variables les plus positivement corr√©l√©es\n",
    "print(\"\\nVariables les plus positivement corr√©l√©es √† TARGET:\")\n",
    "print(corr_train.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corr√©lation pour les variables num√©riques\n",
    "plt.figure(figsize=(16, 14))\n",
    "# S√©lectionner uniquement les colonnes num√©riques\n",
    "numeric_vars = train.select_dtypes(include=[np.number]).columns\n",
    "# Limiter √† 20 variables pour une meilleure lisibilit√©\n",
    "selected_vars = list(numeric_vars[:20])\n",
    "if 'TARGET' in train.columns and 'TARGET' not in selected_vars:\n",
    "    selected_vars.append('TARGET')\n",
    "sns.heatmap(train[selected_vars].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Matrice de corr√©lation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorer les corr√©lations avec la variable cible\n",
    "# Utiliser numeric_only=True pour √©viter l'erreur avec les colonnes non num√©riques\n",
    "corr_train = train.corr(numeric_only=True)['TARGET'].sort_values()\n",
    "\n",
    "# Afficher les 10 variables les plus n√©gativement corr√©l√©es\n",
    "print(\"Variables les plus n√©gativement corr√©l√©es √† TARGET:\")\n",
    "print(corr_train.head(10))\n",
    "\n",
    "# Afficher les 10 variables les plus positivement corr√©l√©es\n",
    "print(\"\\nVariables les plus positivement corr√©l√©es √† TARGET:\")\n",
    "print(corr_train.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962711e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation des valeurs manquantes et mise √† l'√©chelle des donn√©es\n",
    "\n",
    "# S√©parer les variables et la cible\n",
    "features = [col for col in train.columns if col != 'TARGET']\n",
    "target = train['TARGET'].values\n",
    "\n",
    "# Remplacer les valeurs infinies par 0\n",
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "test = test.replace([np.inf, -np.inf], np.nan)\n",
    "new_test = new_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Cr√©er un SimpleImputer pour remplacer les valeurs manquantes par la m√©diane\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(train[features])\n",
    "\n",
    "# Transformer les jeux de donn√©es\n",
    "train_transformed = imputer.transform(train[features])\n",
    "test_transformed = imputer.transform(test[features])\n",
    "new_test_transformed = imputer.transform(new_test)\n",
    "\n",
    "# Mise √† l'√©chelle des donn√©es\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_transformed = scaler.fit_transform(train_transformed)\n",
    "test_transformed = scaler.transform(test_transformed)\n",
    "new_test_transformed = scaler.transform(new_test_transformed)\n",
    "\n",
    "print(f\"Dimensions apr√®s transformation:\")\n",
    "print(f\"Train transform√©: {train_transformed.shape}\")\n",
    "print(f\"Test transform√©: {test_transformed.shape}\")\n",
    "print(f\"New test transform√©: {new_test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation de variables bas√©es sur l'expertise du domaine\n",
    "# Ces variables sont mentionn√©es dans l'article de Wells Fargo sur les facteurs de cr√©dit\n",
    "\n",
    "# debt-to-income ratio (DIR) = Montant du cr√©dit / Revenu total\n",
    "train['DIR'] = train['AMT_CREDIT'] / train['AMT_INCOME_TOTAL']\n",
    "test['DIR'] = test['AMT_CREDIT'] / test['AMT_INCOME_TOTAL']\n",
    "new_test['DIR'] = new_test['AMT_CREDIT'] / new_test['AMT_INCOME_TOTAL']\n",
    "\n",
    "# annuity-to-income ratio (AIR) = Annuit√© du pr√™t / Revenu total\n",
    "train['AIR'] = train['AMT_ANNUITY'] / train['AMT_INCOME_TOTAL']\n",
    "test['AIR'] = test['AMT_ANNUITY'] / test['AMT_INCOME_TOTAL']\n",
    "new_test['AIR'] = new_test['AMT_ANNUITY'] / new_test['AMT_INCOME_TOTAL']\n",
    "\n",
    "# annuity-to-credit ratio (ACR) = Annuit√© du pr√™t / Montant du cr√©dit\n",
    "train['ACR'] = train['AMT_ANNUITY'] / train['AMT_CREDIT']\n",
    "test['ACR'] = test['AMT_ANNUITY'] / test['AMT_CREDIT']\n",
    "new_test['ACR'] = new_test['AMT_ANNUITY'] / new_test['AMT_CREDIT']\n",
    "\n",
    "# days-employed-to-age ratio (DAR) = Jours employ√©s / √Çge du demandeur\n",
    "train['DAR'] = train['DAYS_EMPLOYED'] / train['DAYS_BIRTH']\n",
    "test['DAR'] = test['DAYS_EMPLOYED'] / test['DAYS_BIRTH']\n",
    "new_test['DAR'] = new_test['DAYS_EMPLOYED'] / new_test['DAYS_BIRTH']\n",
    "\n",
    "# V√©rifier les corr√©lations des nouvelles variables avec TARGET\n",
    "domain_features = ['DIR', 'AIR', 'ACR', 'DAR']\n",
    "domain_corrs = train[domain_features + ['TARGET']].corr(numeric_only=True)['TARGET']\n",
    "print(\"Corr√©lations des variables d'expertise de domaine avec TARGET:\")\n",
    "print(domain_corrs[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ee09c",
   "metadata": {},
   "source": [
    "## Pr√©paration des donn√©es pour la mod√©lisation\n",
    "\n",
    "Avant de construire des mod√®les, nous devons pr√©parer les donn√©es en encodant les variables cat√©gorielles et en traitant les valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder les variables cat√©gorielles avec 2 cat√©gories ou moins\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "transform_counter = 0\n",
    "\n",
    "# V√©rifier si le jeu de donn√©es contient des variables de type 'object'\n",
    "if 'object' in train.dtypes.values:\n",
    "    # It√©rer √† travers toutes les colonnes cat√©gorielles\n",
    "    for col in train.select_dtypes('object').columns:\n",
    "        # S√©lectionner uniquement les colonnes o√π le nombre de valeurs uniques est inf√©rieur ou √©gal √† 2\n",
    "        if train[col].nunique() <= 2:\n",
    "            train[col] = le.fit_transform(train[col].astype(str))\n",
    "            # V√©rifier si la colonne existe dans les jeux de test\n",
    "            if col in test.columns:\n",
    "                test[col] = le.transform(test[col].astype(str))\n",
    "            if col in new_test.columns:\n",
    "                new_test[col] = le.transform(new_test[col].astype(str))\n",
    "            transform_counter += 1\n",
    "    \n",
    "    print(f\"Encodage par √©tiquettes appliqu√© √† {transform_counter} colonnes.\")\n",
    "    \n",
    "    # One-hot encoding pour les variables cat√©gorielles restantes\n",
    "    train = pd.get_dummies(train, drop_first=True)\n",
    "    test = pd.get_dummies(test, drop_first=True)\n",
    "    new_test = pd.get_dummies(new_test, drop_first=True)\n",
    "    \n",
    "    # Aligner les colonnes entre train et test\n",
    "    target = train['TARGET']\n",
    "    train, test = train.align(test, join='inner', axis=1)\n",
    "    train['TARGET'] = target\n",
    "    \n",
    "    # S'assurer que new_test a les m√™mes colonnes que train\n",
    "    for col in train.columns:\n",
    "        if col != 'TARGET' and col not in new_test.columns:\n",
    "            new_test[col] = 0\n",
    "    new_test = new_test[train.columns[train.columns != 'TARGET']]\n",
    "    \n",
    "    print(f\"Dimensions apr√®s encodage one-hot:\")\n",
    "    print(f\"Train: {train.shape}\")\n",
    "    print(f\"Test: {test.shape}\")\n",
    "    print(f\"New test: {new_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corr√©lation pour les variables num√©riques\n",
    "plt.figure(figsize=(16, 14))\n",
    "numeric_vars = train.select_dtypes(include=[np.number]).columns\n",
    "# Limiter √† 20 variables pour une meilleure lisibilit√©\n",
    "selected_vars = list(numeric_vars[:20])\n",
    "if 'TARGET' in train.columns and 'TARGET' not in selected_vars:\n",
    "    selected_vars.append('TARGET')\n",
    "sns.heatmap(train[selected_vars].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Matrice de corr√©lation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc465ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer les pr√©dictions de probabilit√© pour la soumission\n",
    "log_regression_pred_test = logistic_regressor.predict_proba(test_transformed)[:, 1]\n",
    "\n",
    "# Cr√©er un DataFrame pour la soumission\n",
    "submission_log_regression = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': log_regression_pred_test})\n",
    "submission_log_regression.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c833d3",
   "metadata": {},
   "source": [
    "## Mod√©lisation\n",
    "\n",
    "Nous allons entra√Æner plusieurs mod√®les de classification et √©valuer leurs performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement du mod√®le XGBoost (conditionnel)\n",
    "if 'XGB_AVAILABLE' in globals() and XGB_AVAILABLE:\n",
    "    xgb_classifier = XGBClassifier(n_estimators=250, max_depth=5)\n",
    "    xgb_classifier.fit(X_training_set, y_training_set)\n",
    "    \n",
    "    # Pr√©dictions sur l'ensemble de validation\n",
    "    xgb_pred = xgb_classifier.predict(X_validation_set)\n",
    "    xgb_pred_proba = xgb_classifier.predict_proba(X_validation_set)[:, 1]\n",
    "    \n",
    "    # √âvaluation du mod√®le\n",
    "    print(\"XGBoost - Rapport de classification:\")\n",
    "    print(classification_report(y_validation_set, xgb_pred))\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_validation_set, xgb_pred_proba):.4f}\")\n",
    "    \n",
    "    # Pr√©dictions sur le nouveau jeu de test\n",
    "    xgb_new = xgb_classifier.predict(new_test_transformed)\n",
    "    print(\"\\nDistribution des pr√©dictions sur le nouveau jeu de test:\")\n",
    "    print(pd.Series(xgb_new).value_counts())\n",
    "else:\n",
    "    print(\"XGBoost n'est pas disponible. Passage √† l'√©tape suivante.\")\n",
    "    # D√©finir des valeurs vides pour ne pas casser le code plus tard\n",
    "    xgb_classifier = None\n",
    "    xgb_pred = np.zeros(len(y_validation_set))\n",
    "    xgb_pred_proba = np.zeros(len(y_validation_set))\n",
    "    xgb_new = np.zeros(len(new_test_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'importance des caract√©ristiques du mod√®le XGBoost\n",
    "xgb_importance = plot_importance(xgb_classifier, features, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0563ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer les pr√©dictions de probabilit√© pour la soumission\n",
    "log_regression_pred_test = logistic_regressor.predict_proba(test_transformed)[:, 1]\n",
    "\n",
    "# Cr√©er un DataFrame pour la soumission\n",
    "submission_log_regression = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': log_regression_pred_test})\n",
    "submission_log_regression.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49a22b",
   "metadata": {},
   "source": [
    "### 2. Random Forest (For√™t al√©atoire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e56692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement du mod√®le Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n",
    "random_forest.fit(X_training_set, y_training_set)\n",
    "\n",
    "# Pr√©dictions sur l'ensemble de validation\n",
    "random_forest_pred = random_forest.predict(X_validation_set)\n",
    "random_forest_pred_proba = random_forest.predict_proba(X_validation_set)[:, 1]\n",
    "\n",
    "# √âvaluation du mod√®le\n",
    "print(\"Random Forest - Rapport de classification:\")\n",
    "print(classification_report(y_validation_set, random_forest_pred))\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_validation_set, random_forest_pred_proba):.4f}\")\n",
    "\n",
    "# Pr√©dictions sur le nouveau jeu de test\n",
    "random_forest_new = random_forest.predict(new_test_transformed)\n",
    "print(\"\\nDistribution des pr√©dictions sur le nouveau jeu de test:\")\n",
    "print(pd.Series(random_forest_new).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbd59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser l'importance des caract√©ristiques\n",
    "def plot_importance(model, features, top_n=20):\n",
    "    \"\"\"Affiche un graphique des caract√©ristiques les plus importantes.\"\"\"\n",
    "    # Cr√©er un DataFrame pour l'importance des caract√©ristiques\n",
    "    feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': model.feature_importances_})\n",
    "    feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Cr√©er le graphique\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(top_n))\n",
    "    plt.title(f\"Les {top_n} caract√©ristiques les plus importantes\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importance_df\n",
    "\n",
    "# Visualiser l'importance des caract√©ristiques du mod√®le Random Forest\n",
    "rf_importance = plot_importance(random_forest, features, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f45538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier l'importance des caract√©ristiques cr√©√©es par expertise de domaine\n",
    "domain_features = ['DIR', 'AIR', 'ACR', 'DAR']\n",
    "domain_indices = [i for i, feature in enumerate(features) if feature in domain_features]\n",
    "\n",
    "if domain_indices:\n",
    "    domain_importance = pd.DataFrame({\n",
    "        'Feature': [features[i] for i in domain_indices],\n",
    "        'Importance': [random_forest.feature_importances_[i] for i in domain_indices]\n",
    "    })\n",
    "    domain_importance = domain_importance.sort_values('Importance', ascending=False)\n",
    "    print(\"Importance des caract√©ristiques cr√©√©es par expertise de domaine:\")\n",
    "    print(domain_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baffdd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer les pr√©dictions pour la soumission\n",
    "random_forest_pred_test = random_forest.predict_proba(test_transformed)[:, 1]\n",
    "submission_rf = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': random_forest_pred_test})\n",
    "submission_rf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c69dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer les pr√©dictions pour la soumission\n",
    "xgb_pred_test = xgb_classifier.predict_proba(test_transformed)[:, 1]\n",
    "submission_xgb = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': xgb_pred_test})\n",
    "submission_xgb.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a723f4b",
   "metadata": {},
   "source": [
    "### 4. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31246bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de base pour LightGBM (conditionnel)\n",
    "if 'LGBM_AVAILABLE' in globals() and LGBM_AVAILABLE:\n",
    "    def get_lgbm_params():\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': 100,\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 40,\n",
    "            'max_depth': -1,  # -1 signifie pas de limite\n",
    "            'subsample': 1.0,\n",
    "            'colsample_bytree': 1.0,\n",
    "            'reg_alpha': 0.0,\n",
    "            'reg_lambda': 0.0,\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 50\n",
    "        }\n",
    "        return params\n",
    "    \n",
    "    # Fonction d'√©valuation avec arr√™t pr√©coce\n",
    "    def train_lgbm_model(X_train, y_train, X_val, y_val, params):\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        \n",
    "        # Configuration de l'arr√™t pr√©coce\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=10\n",
    "        )\n",
    "        \n",
    "        # √âvaluation sur l'ensemble de validation\n",
    "        val_pred = model.predict_proba(X_val)[:, 1]\n",
    "        val_auc = roc_auc_score(y_val, val_pred)\n",
    "        print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        return model, val_auc\n",
    "    \n",
    "    # Diviser les donn√©es d'entra√Ænement pour la validation\n",
    "    X_train_lgb, X_val, y_train_lgb, y_val = train_test_split(X_training_set, y_training_set, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Entra√Æner le mod√®le LightGBM\n",
    "    lgb_params = get_lgbm_params()\n",
    "    lgb_model, val_auc = train_lgbm_model(X_train_lgb, y_train_lgb, X_val, y_val, lgb_params)\n",
    "else:\n",
    "    print(\"LightGBM n'est pas disponible. Passage √† l'√©tape suivante.\")\n",
    "    # D√©finir des valeurs vides\n",
    "    lgb_model = None\n",
    "    lgb_pred = np.zeros(len(y_validation_set))\n",
    "    lgb_pred_proba = np.zeros(len(y_validation_set))\n",
    "    lgb_new = np.zeros(len(new_test_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922801e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluer le mod√®le LightGBM sur l'ensemble de validation\n",
    "lgb_pred = lgb_model.predict(X_validation_set)\n",
    "lgb_pred_proba = lgb_model.predict_proba(X_validation_set)[:, 1]\n",
    "\n",
    "print(\"LightGBM - Rapport de classification:\")\n",
    "print(classification_report(y_validation_set, lgb_pred))\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_validation_set, lgb_pred_proba):.4f}\")\n",
    "\n",
    "# Pr√©dictions sur le nouveau jeu de test\n",
    "lgb_new = lgb_model.predict(new_test_transformed)\n",
    "print(\"\\nDistribution des pr√©dictions sur le nouveau jeu de test:\")\n",
    "print(pd.Series(lgb_new).value_counts())\n",
    "\n",
    "# Visualiser l'importance des caract√©ristiques\n",
    "lgb_importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': lgb_model.feature_importances_ / sum(lgb_model.feature_importances_)\n",
    "})\n",
    "lgb_importance = plot_importance(lgb_model, features, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer les pr√©dictions pour la soumission\n",
    "lgb_pred_test = lgb_model.predict_proba(test_transformed)[:, 1]\n",
    "submission_lgb = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': lgb_pred_test})\n",
    "submission_lgb.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9585558b",
   "metadata": {},
   "source": [
    "### 5. Mod√®le d'ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f111db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un mod√®le d'ensemble par vote majoritaire\n",
    "def stacked_model(predictions):\n",
    "    \"\"\"Effectue un vote majoritaire sur les pr√©dictions de plusieurs mod√®les.\"\"\"\n",
    "    stacked_predictions = np.array([])\n",
    "    \n",
    "    for element in predictions:\n",
    "        stacked_predictions = np.append(stacked_predictions, stats.mode(element)[0][0])\n",
    "        \n",
    "    return stacked_predictions\n",
    "\n",
    "# Combiner toutes les pr√©dictions en un tableau multidimensionnel\n",
    "combined_array = np.column_stack([\n",
    "    log_regression_pred,\n",
    "    xgb_pred,\n",
    "    lgb_pred,\n",
    "    random_forest_pred\n",
    "])\n",
    "\n",
    "# Faire des pr√©dictions avec le mod√®le par ensemble\n",
    "stacked_model_pred = stacked_model(combined_array)\n",
    "\n",
    "# √âvaluer le mod√®le d'ensemble\n",
    "print(\"Mod√®le d'ensemble - Rapport de classification:\")\n",
    "print(classification_report(y_validation_set, stacked_model_pred))\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_validation_set, stacked_model_pred):.4f}\")\n",
    "\n",
    "# Pr√©dictions sur le nouveau jeu de test\n",
    "combined_new = np.column_stack([\n",
    "    logistic_new,\n",
    "    xgb_new,\n",
    "    lgb_new,\n",
    "    random_forest_new\n",
    "])\n",
    "stacked_new = stacked_model(combined_new).astype(int)\n",
    "print(\"\\nDistribution des pr√©dictions d'ensemble sur le nouveau jeu de test:\")\n",
    "print(pd.Series(stacked_new).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26fa519",
   "metadata": {},
   "source": [
    "## S√©lection et sauvegarde du meilleur mod√®le\n",
    "\n",
    "Nous allons sauvegarder notre meilleur mod√®le au format .h5 pour l'utiliser ult√©rieurement dans l'application web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57df10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluer tous les mod√®les disponibles sur l'ensemble de validation\n",
    "models = {\n",
    "    'Logistic Regression': {'model': logistic_regressor, 'auc': roc_auc_score(y_validation_set, log_regression_pred_proba)}\n",
    "}\n",
    "\n",
    "# Ajouter Random Forest si disponible\n",
    "if 'random_forest' in globals() and random_forest is not None:\n",
    "    models['Random Forest'] = {\n",
    "        'model': random_forest, \n",
    "        'auc': roc_auc_score(y_validation_set, random_forest_pred_proba)\n",
    "    }\n",
    "\n",
    "# Ajouter XGBoost si disponible\n",
    "if 'XGB_AVAILABLE' in globals() and XGB_AVAILABLE and xgb_classifier is not None:\n",
    "    models['XGBoost'] = {\n",
    "        'model': xgb_classifier, \n",
    "        'auc': roc_auc_score(y_validation_set, xgb_pred_proba)\n",
    "    }\n",
    "\n",
    "# Ajouter LightGBM si disponible\n",
    "if 'LGBM_AVAILABLE' in globals() and LGBM_AVAILABLE and lgb_model is not None:\n",
    "    models['LightGBM'] = {\n",
    "        'model': lgb_model, \n",
    "        'auc': roc_auc_score(y_validation_set, lgb_pred_proba)\n",
    "    }\n",
    "\n",
    "# Trouver le meilleur mod√®le\n",
    "best_model_name = max(models, key=lambda x: models[x]['auc'])\n",
    "best_model = models[best_model_name]['model']\n",
    "best_auc = models[best_model_name]['auc']\n",
    "\n",
    "print(f\"Le meilleur mod√®le est {best_model_name} avec un AUC-ROC de {best_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c10cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le r√©pertoire models s'il n'existe pas\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Fonction pour sauvegarder diff√©rents types de mod√®les au format h5\n",
    "def save_model_as_h5(model, model_name):\n",
    "    \"\"\"\n",
    "    Sauvegarde un mod√®le au format h5 selon son type\n",
    "    \"\"\"\n",
    "    file_path = f'../models/{model_name}.h5'\n",
    "    \n",
    "    if 'LGBM_AVAILABLE' in globals() and LGBM_AVAILABLE and isinstance(model, lgb.LGBMModel):  # LightGBM models\n",
    "        model.booster_.save_model(file_path)\n",
    "        print(f\"Mod√®le LightGBM sauvegard√© dans {file_path}\")\n",
    "        \n",
    "    elif 'XGB_AVAILABLE' in globals() and XGB_AVAILABLE and isinstance(model, XGBClassifier):  # XGBoost models\n",
    "        model.save_model(file_path)\n",
    "        print(f\"Mod√®le XGBoost sauvegard√© dans {file_path}\")\n",
    "        \n",
    "    else:  # Scikit-learn models (RandomForest, LogisticRegression, etc.)\n",
    "        # Essayer de sauvegarder au format h5 si TensorFlow est disponible\n",
    "        if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "            from tensorflow.keras.models import Sequential\n",
    "            from tensorflow.keras.layers import Dense\n",
    "            \n",
    "            # Sauvegarder d'abord le mod√®le sklearn en utilisant pickle temporairement\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False).name\n",
    "            with open(temp_file, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "                \n",
    "            # Cr√©er un mod√®le keras qui servira de conteneur\n",
    "            container_model = Sequential()\n",
    "            container_model.add(Dense(1, input_shape=(1,)))\n",
    "            \n",
    "            # Sauvegarder au format h5\n",
    "            container_model.save(file_path)\n",
    "            \n",
    "            # Ajouter le mod√®le sklearn comme attribut personnalis√©\n",
    "            with h5py.File(file_path, 'a') as h5file:\n",
    "                sklearn_group = h5file.create_group('sklearn_model')\n",
    "                with open(temp_file, 'rb') as f:\n",
    "                    sklearn_group.create_dataset('model_dump', data=np.void(f.read()))\n",
    "                    \n",
    "            # Supprimer le fichier temporaire\n",
    "            os.remove(temp_file)\n",
    "            print(f\"Mod√®le {model.__class__.__name__} sauvegard√© dans {file_path}\")\n",
    "        else:\n",
    "            # Utiliser pickle si TensorFlow n'est pas disponible\n",
    "            with open(file_path.replace('.h5', '.pkl'), 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            print(f\"TensorFlow n'est pas disponible. Mod√®le sauvegard√© au format pickle: {file_path.replace('.h5', '.pkl')}\")\n",
    "    \n",
    "    # Sauvegarder √©galement les m√©tadonn√©es du mod√®le\n",
    "    metadata = {\n",
    "        'model_type': model.__class__.__name__,\n",
    "        'auc_score': models[best_model_name]['auc'],\n",
    "        'features': features\n",
    "    }\n",
    "    \n",
    "    with h5py.File(f'../models/{model_name}_metadata.h5', 'w') as h5file:\n",
    "        h5file.attrs['model_type'] = metadata['model_type']\n",
    "        h5file.attrs['auc_score'] = metadata['auc_score']\n",
    "        feature_group = h5file.create_group('features')\n",
    "        for i, feature in enumerate(features):\n",
    "            feature_group.attrs[f'feature_{i}'] = feature\n",
    "            \n",
    "    return file_path\n",
    "\n",
    "# Sauvegarder le meilleur mod√®le au format h5 ou pickle\n",
    "model_path = save_model_as_h5(best_model, best_model_name.lower().replace(' ', '_'))\n",
    "\n",
    "# Sauvegarder √©galement l'imputer et le scaler pour une utilisation future\n",
    "with h5py.File('../models/preprocessing.h5', 'w') as h5file:\n",
    "    # Sauvegarder l'imputer et le scaler en tant qu'attributs personnalis√©s\n",
    "    imputer_temp = tempfile.NamedTemporaryFile(delete=False).name\n",
    "    scaler_temp = tempfile.NamedTemporaryFile(delete=False).name\n",
    "    \n",
    "    with open(imputer_temp, 'wb') as f:\n",
    "        pickle.dump(imputer, f)\n",
    "    with open(scaler_temp, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "        \n",
    "    # Cr√©er des groupes pour l'imputer et le scaler\n",
    "    imputer_group = h5file.create_group('imputer')\n",
    "    scaler_group = h5file.create_group('scaler')\n",
    "    \n",
    "    # Ajouter les donn√©es s√©rialis√©es\n",
    "    with open(imputer_temp, 'rb') as f:\n",
    "        imputer_group.create_dataset('data', data=np.void(f.read()))\n",
    "    with open(scaler_temp, 'rb') as f:\n",
    "        scaler_group.create_dataset('data', data=np.void(f.read()))\n",
    "        \n",
    "    # Supprimer les fichiers temporaires\n",
    "    os.remove(imputer_temp)\n",
    "    os.remove(scaler_temp)\n",
    "    \n",
    "print(\"Pr√©processeurs sauvegard√©s dans ../models/preprocessing.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefacf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour charger un mod√®le h5 selon son type\n",
    "def load_model_from_h5(file_path):\n",
    "    \"\"\"\n",
    "    Charge un mod√®le √† partir d'un fichier h5\n",
    "    \"\"\"\n",
    "    # D'abord, d√©terminer le type de mod√®le √† partir des m√©tadonn√©es\n",
    "    metadata_path = file_path.replace('.h5', '_metadata.h5')\n",
    "    \n",
    "    with h5py.File(metadata_path, 'r') as h5file:\n",
    "        model_type = h5file.attrs['model_type']\n",
    "    \n",
    "    if model_type == 'LGBMClassifier':  # LightGBM models\n",
    "        model = lgb.Booster(model_file=file_path)\n",
    "        # Convertir le booster en LGBMClassifier\n",
    "        classifier = lgb.LGBMClassifier()\n",
    "        classifier._Booster = model\n",
    "        return classifier\n",
    "        \n",
    "    elif model_type == 'XGBClassifier':  # XGBoost models\n",
    "        model = XGBClassifier()\n",
    "        model.load_model(file_path)\n",
    "        return model\n",
    "        \n",
    "    else:  # Scikit-learn models (RandomForest, LogisticRegression, etc.)\n",
    "        # Charger le mod√®le depuis le format h5\n",
    "        with h5py.File(file_path, 'r') as h5file:\n",
    "            sklearn_group = h5file['sklearn_model']\n",
    "            model_dump = sklearn_group['model_dump'][()]\n",
    "            \n",
    "            # √âcrire le dump dans un fichier temporaire\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False).name\n",
    "            with open(temp_file, 'wb') as f:\n",
    "                f.write(model_dump.tobytes())\n",
    "                \n",
    "            # Charger le mod√®le depuis le fichier temporaire\n",
    "            with open(temp_file, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "                \n",
    "            # Supprimer le fichier temporaire\n",
    "            os.remove(temp_file)\n",
    "            \n",
    "        return model\n",
    "\n",
    "# Test: chargement du mod√®le sauvegard√©\n",
    "try:\n",
    "    loaded_model = load_model_from_h5(model_path)\n",
    "    print(f\"Mod√®le charg√© avec succ√®s : {type(loaded_model).__name__}\")\n",
    "    \n",
    "    # V√©rifier que le mod√®le fonctionne correctement\n",
    "    sample_pred = loaded_model.predict_proba(X_validation_set[:5])[:, 1]\n",
    "    print(f\"Pr√©dictions d'√©chantillon: {sample_pred}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement du mod√®le: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae2488",
   "metadata": {},
   "source": [
    "## Conclusion et prochaines √©tapes\n",
    "\n",
    "Dans ce notebook, nous avons:\n",
    "\n",
    "1. Explor√© et pr√©par√© les donn√©es Home Credit pour la mod√©lisation\n",
    "2. Cr√©√© de nouvelles caract√©ristiques via l'ing√©nierie des caract√©ristiques\n",
    "3. Entra√Æn√© et √©valu√© plusieurs mod√®les de machine learning\n",
    "4. Compar√© leurs performances et identifi√© le meilleur mod√®le\n",
    "\n",
    "### Observations cl√©s\n",
    "\n",
    "- XGBoost et LightGBM ont g√©n√©ralement montr√© les meilleures performances\n",
    "- Les caract√©ristiques EXT_SOURCE sont parmi les plus importantes pour la pr√©diction\n",
    "- Notre mod√®le d'ensemble a am√©lior√© l√©g√®rement les performances par rapport aux mod√®les individuels\n",
    "\n",
    "### Prochaines √©tapes\n",
    "\n",
    "1. **API REST** : D√©velopper une API Flask pour servir notre meilleur mod√®le\n",
    "2. **Dashboard** : Cr√©er un tableau de bord interactif avec Dash pour visualiser les pr√©dictions et les explications\n",
    "3. **Optimisation** : Am√©liorer notre mod√®le en ajustant davantage les hyperparam√®tres et en int√©grant plus de donn√©es\n",
    "4. **Interpr√©tabilit√©** : Impl√©menter des m√©thodes comme SHAP pour expliquer les pr√©dictions individuelles\n",
    "5. **D√©ploiement** : Mettre en place l'infrastructure pour d√©ployer le mod√®le et le tableau de bord en production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35203ea5",
   "metadata": {},
   "source": [
    "### 2. Random Forest (For√™t al√©atoire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94071718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement du mod√®le Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n",
    "random_forest.fit(X_training_set, y_training_set)\n",
    "\n",
    "# Pr√©dictions sur l'ensemble de validation\n",
    "random_forest_pred = random_forest.predict(X_validation_set)\n",
    "random_forest_pred_proba = random_forest.predict_proba(X_validation_set)[:, 1]\n",
    "\n",
    "# √âvaluation du mod√®le\n",
    "print(\"Random Forest - Rapport de classification:\")\n",
    "print(classification_report(y_validation_set, random_forest_pred))\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_validation_set, random_forest_pred_proba):.4f}\")\n",
    "\n",
    "# Pr√©dictions sur le nouveau jeu de test\n",
    "random_forest_new = random_forest.predict(new_test_transformed)\n",
    "print(\"\\nDistribution des pr√©dictions sur le nouveau jeu de test:\")\n",
    "print(pd.Series(random_forest_new).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d2f1d",
   "metadata": {},
   "source": [
    "## 7.1 Comparaison des performances avec le mod√®le LightGBM\n",
    "\n",
    "Comparons les performances de notre meilleur mod√®le avec le mod√®le LightGBM de r√©f√©rence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
